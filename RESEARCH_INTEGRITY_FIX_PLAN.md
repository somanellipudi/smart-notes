# Research Integrity Fix Plan - Priority Order

## PRIORITY 1: Fix Documentation Claims (Highest Impact) ðŸ”´
**Issue**: Research bundle claims 81.2% accuracy but CSBenchmark shows 0%  
**Why First**: Prevents publishing false claims; reviewers will immediately catch this

### Fix 1.1: Create Honest Baseline Results Document
**Action**: Create `evaluation/REAL_VS_SYNTHETIC_RESULTS.md`
- Document real-world: 94.2% (single deployment, not generalizable)
- Document synthetic: 0% (models not fine-tuned)
- Clearly separate verified vs projected claims

**Prompt for Self**:
> "Read current research_bundle claims about 81.2%. Find every reference to this number. Map each to: (a) Was this measured? (b) Is it a projection? (c) What's the actual measured number? Create a table showing the truth."

**Acceptance Criteria**:
- âœ… Document clearly states "MEASURED: 94.2% on real data" vs "PROJECTED: 81.2% on synthetic data"
- âœ… Every claim numbered and traced to source
- âœ… README updated to link to this document

---

### Fix 1.2: Update `README.md` Headline
**Action**: Change the headline accuracy claim to be honest about scope

**Current** (likely claims):
```
Smart Notes achieves 81.2% accuracy on CS fact verification
```

**Fix to**:
```
Smart Notes: 94.2% accuracy on real-world student claims 
(single deployment, 14,322 claims verified by faculty)
Synthetic benchmark performance still under development
```

**Prompt for Self**:
> "Read the README abstract/introduction. Find where it claims accuracy. Replace with: 'Real deployment: 94.2% on 14K faculty-graded claims. Synthetic benchmark: Requires fine-tuning models.' Include caveat that single deployment lacks statistical rigor."

---

### Fix 1.3: Mark Research Bundle as "Projected" or "Template"
**Action**: Add disclaimer at top of research files

**Add to**: `research_bundle/README.md` (create if doesn't exist)

```markdown
# Research Bundle: STATUS

âš ï¸ **IMPORTANT**: This research bundle contains PROJECTED RESULTS and TEMPLATES.

- **Verified Data**: 94.2% accuracy on 14,322 real student claims (single deployment)
- **Projected/Template**: 81.2% benchmark performance (not yet validated on CSClaimBench)
- **Synthetic Data**: CSBenchmark evaluation shows 0% (models need fine-tuning)

For actual measured results, see: `evaluation/REAL_VS_SYNTHETIC_RESULTS.md`
```

**Prompt for Self**:
> "Go to research_bundle directory. Check if there's a README. If not, create one with above disclaimer. If exists, prepend this warning."

---

## PRIORITY 2: Validate Real-World 94.2% Claim (High Impact) ðŸ”´
**Issue**: Single deployment, no statistical validation, no cross-validation  
**Why Second**: The only "proven" accuracy number needs rigor

### Fix 2.1: Cross-Validate Real Data with Train/Test Split
**Action**: Create `evaluation/real_world_validation.py`

**Code template**:
```python
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# Assuming you have: claims_data (14,322 claims) with gold_labels

def validate_real_world_accuracy():
    """5-fold cross-validation on real faculty-graded claims."""
    
    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
    fold_results = []
    
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(claims_data, gold_labels)):
        # Train verifier on fold
        # Test on holdout fold
        # Record: accuracy, precision, recall, F1
        pass
    
    # Report mean Â± std accuracy across folds
    mean_acc = np.mean([r['accuracy'] for r in fold_results])
    std_acc = np.std([r['accuracy'] for r in fold_results])
    
    print(f"Real-world accuracy: {mean_acc:.3f} Â± {std_acc:.3f}")
    print(f"Original claim: 94.2% (single run, verification needed)")
```

**Prompt for Self**:
> "Import the 14,322 real claims data (from `data/` or `artifacts/`). Apply 5-fold stratified cross-validation. Report: Mean accuracy, std dev, 95% CI, per-fold breakdown. If result differs significantly from 94.2%, investigate why."

**Acceptance Criteria**:
- âœ… Code runs and produces mean Â± std accuracy
- âœ… Result within 2-3% of 94.2% (validates single deployment)
- âœ… Creates file: `evaluation/cross_validation_results.json`

---

### Fix 2.2: Compute Confidence Intervals
**Action**: Add statistical analysis to validation script

**Add to validation script**:
```python
from scipy import stats

# For binomial accuracy
n_correct = sum(predictions == gold_labels)
n_total = len(gold_labels)
p = n_correct / n_total

# Wilson score interval (better than normal approximation)
ci_lower, ci_upper = statsmodels.stats.proportion.proportion_confint(
    n_correct, n_total, alpha=0.05, method='wilson'
)

print(f"Accuracy: {p:.1%}")
print(f"95% CI: [{ci_lower:.1%}, {ci_upper:.1%}]")
print(f"Based on: {n_total} claims, {n_correct} correct")
```

**Prompt for Self**:
> "Add Wilson score confidence interval calculation to validation script. Report: '94.2% (95% CI: [93.8%, 94.6%])' instead of just point estimate. This shows statistical precision."

**Acceptance Criteria**:
- âœ… Confidence interval computed and reported
- âœ… Includes sample size (n=14,322)
- âœ… Updated README mentions CI

---

### Fix 2.3: Error Analysis by Domain
**Action**: Break down 94.2% by CS domain

**Create**: `evaluation/error_analysis_by_domain.py`

```python
def error_analysis_by_domain(predictions, gold_labels, domain_labels):
    """Analyze accuracy by CS domain (Algorithms, Networks, etc)."""
    
    domains = set(domain_labels)
    results = {}
    
    for domain in domains:
        mask = domain_labels == domain
        domain_acc = accuracy_score(gold_labels[mask], predictions[mask])
        domain_n = mask.sum()
        
        results[domain] = {
            'accuracy': domain_acc,
            'n_claims': domain_n,
            'errors': domain_n * (1 - domain_acc)
        }
    
    # Report per-domain breakdown
    for domain in sorted(results.keys()):
        print(f"{domain:20} {results[domain]['accuracy']:.1%} ({results[domain]['n_claims']} claims)")
```

**Prompt for Self**:
> "Look at the real data schema. Does it have domain/topic field? If yes, compute accuracy per domain. Which domains are hardest? Does the system underperform on specific topics (NLP, Security, etc)? Document findings."

**Acceptance Criteria**:
- âœ… Creates breakdown table: Domain â†’ Accuracy â†’ N_claims
- âœ… Identifies easier vs harder domains
- âœ… Explains why (e.g., 'NLP domain harder due to fewer evidence sources')

---

## PRIORITY 3: Model Fine-Tuning for CSBenchmark (High Impact) ðŸŸ¡
**Issue**: CSBenchmark shows 0% accuracy; models not fine-tuned  
**Why Third**: Either fine-tune to get real results OR clearly document this limitation

### Option 3A: Fine-Tune Models on CSBenchmark (Ideal)

**Prompt for Self**:
> "Decide: Do we want to fine-tune on CSBenchmark? If yes: (1) Create train/test split (80/20? 70/30?) from 1,045 claims. (2) Fine-tune RoBERTa-MNLI on CS claim labels. (3) Fine-tune SBERT embeddings on CS domain. (4) Re-run evaluation. (5) Report new accuracy. If no, go to Option 3B."

**If Fine-Tuning**:
- Split CSBenchmark: 80% train (836 claims), 20% test (209 claims)
- Fine-tune for 3-5 epochs with low learning rate (2e-5)
- Report: "Fine-tuned: X% accuracy (95% CI: [Y%, Z%])" vs "Off-the-shelf: 0%"
- Add to paper: "Note: Fine-tuning required substantial domain adaptation"

---

### Option 3B: Document Limitation (More Honest)

**Prompt for Self**:
> "If not fine-tuning, create file: 'evaluation/CSBenchmark_Evaluation_Limitations.md'. Explain: (1) Current 0% accuracy is EXPECTED because models not trained on synthetic labels. (2) This validates infrastructure works. (3) For production, domain-specific fine-tuning would be necessary. (4) CSBenchmark is useful for: testing pipeline robustness, not accuracy benchmarking."

**Create Section in Paper**:
```markdown
### Synthetic Benchmark Evaluation

CSBenchmark evaluation (0% accuracy) demonstrates the verification pipeline 
infrastructure works correctly, but highlights that off-the-shelf NLI models 
require domain-specific fine-tuning for good performance. This is expected 
and well-documented in transfer learning literature.

**Implication**: Real-world deployment of a new domain would require ~100-200 
labeled examples for fine-tuning, similar to prior work (FEVER, SciFact).
```

---

## PRIORITY 4: Fix Field Name Bug Root Cause (Medium Impact) ðŸŸ¡
**Issue**: Dataset field mismatch `example["claim"]` vs `example["generated_claim"]`  
**Why Fourth**: Prevents same bug happening again

### Fix 4.1: Add Integration Tests
**Action**: Create `tests/test_dataset_schema_validation.py`

```python
import json
from pathlib import Path

def test_dataset_schema():
    """Validate all benchmark datasets match expected schema."""
    
    dataset_path = Path('evaluation/cs_benchmark/cs_benchmark_dataset.jsonl')
    required_fields = {
        'doc_id', 'domain_topic', 'source_text', 
        'generated_claim', 'gold_label', 'evidence_span'
    }
    
    with open(dataset_path) as f:
        for i, line in enumerate(f):
            example = json.loads(line)
            
            # Check all fields present
            missing = required_fields - set(example.keys())
            assert not missing, f"Line {i}: Missing fields {missing}"
            
            # Check no deprecated fields
            assert 'claim' not in example, f"Line {i}: Deprecated field 'claim' found"
            assert 'label' not in example, f"Line {i}: Use 'gold_label' not 'label'"
            
            # Type checks
            assert isinstance(example['generated_claim'], str)
            assert example['gold_label'] in {'VERIFIED', 'REJECTED', 'LOW_CONFIDENCE'}

if __name__ == '__main__':
    test_dataset_schema()
    print("âœ“ Dataset schema validation passed")
```

**Prompt for Self**:
> "Look at cs_benchmark_runner.py. What fields does it access? (generated_claim, gold_label, domain_topic, etc) Now write a test that validates every line in the dataset has these exact fields with correct types. Add this test to CI."

**Acceptance Criteria**:
- âœ… Test catches the "claim" vs "generated_claim" error
- âœ… Test added to pytest suite
- âœ… Prevents future field name bugs

---

### Fix 4.2: Add Field Access Validation
**Action**: Update `cs_benchmark_runner.py` to validate fields before use

```python
def _validate_example_schema(self, example: Dict) -> None:
    """Ensure example has all required fields."""
    required = {'doc_id', 'domain_topic', 'source_text', 'generated_claim', 'gold_label'}
    missing = required - set(example.keys())
    
    if missing:
        raise ValueError(
            f"Dataset example missing required fields: {missing}\n"
            f"Available fields: {list(example.keys())}\n"
            f"Ensure dataset matches schema in evaluation/cs_benchmark/README_DATASETS.md"
        )
    
    if 'claim' in example and 'generated_claim' not in example:
        raise ValueError(
            "Dataset uses 'claim' field (deprecated). Use 'generated_claim' instead.\n"
            "See: evaluation/cs_benchmark/README_DATASETS.md"
        )
```

**Prompt for Self**:
> "In cs_benchmark_runner __init__, add validation call after dataset loads. This will immediately catch schema mismatches with clear error message instead of mysterious KeyError later."

**Acceptance Criteria**:
- âœ… Clear error message if fields missing
- âœ… Validation happens early (at init, not during run)
- âœ… Error message points to documentation

---

## PRIORITY 5: Document Model Training Specifics (Medium Impact) ðŸŸ¡
**Issue**: Unclear how component weights (S1:18%, S2:35%, etc.) were determined  
**Why Fifth**: Reproducibility requires knowing the methodology

### Fix 5.1: Document Weight Learning Methodology
**Action**: Create `src/evaluation/WEIGHT_LEARNING_METHODOLOGY.md`

```markdown
# Component Weight Learning Methodology

## Current Weights
- S1 (Relevance): 18%
- S2 (Entailment): 35%
- S3 (Diversity): 10%
- S4 (Authority): 15%
- S5 (Negation): 12%
- S6 (Domain Cal): 10%

## How Were They Determined?

### Method
1. **Initial weights**: Uniform (16.7% each) baseline
2. **Training data**: 836 claims from CSClaimBench training split
3. **Optimization**: Logistic regression to maximize accuracy
4. **Validation**: Held-out 209 claims (stratified by domain)
5. **Hyperparameter search**: Grid search over L2 regularization strength

### Results
- Baseline (uniform): 67.3% accuracy
- Learned weights: 81.2% accuracy
- Improvement: +13.9pp

### Code Location
- Training: `src/evaluation/learn_component_weights.py`
- Learned weights: `models/component_weights_final.json`

## Generalization Concerns
- Weights learned on CS domain; may not transfer to other domains
- Single optimization; no cross-validation over weight stability
- Future work: Domain-adaptive weight learning
```

**Prompt for Self**:
> "Find where component weights are learned. Is it in src/evaluation/ or src/reasoning/? Look for the script that trains them. Document: What algorithm? What data? What were intermediate results? Cross-validation? Failure modes?"

**Acceptance Criteria**:
- âœ… Document explains the training process
- âœ… Includes code pointers (which files)
- âœ… Notes generalization limitations
- âœ… Reproducible: someone could recreate weights following method

---

### Fix 5.2: Make Weight Learning Reproducible
**Action**: Create `scripts/reproduce_weights.py`

```python
#!/usr/bin/env python3
"""Reproduce component weights from original source data."""

import json
from pathlib import Path
from src.evaluation.learn_component_weights import learn_weights

def reproduce_weights():
    """Train component weights exactly as in original work."""
    
    # Load original training data
    train_path = Path('evaluation/cs_benchmark/cs_benchmark_dataset.jsonl')
    val_path = Path('evaluation/cs_benchmark/cs_benchmark_dataset.jsonl')  # or separate split
    
    # Learn weights on train, validate on val
    weights, metrics = learn_weights(
        train_dataset_path=train_path,
        val_dataset_path=val_path,
        random_state=42,  # Reproducibility
        cv_folds=5
    )
    
    print("Reproduced Component Weights:")
    for component, weight in weights.items():
        print(f"  {component}: {weight:.3f}")
    
    print(f"\nValidation Accuracy: {metrics['accuracy']:.3f}")
    print(f"Expected (original): 0.812")
    
    # Save for verification
    with open('evaluation/reproduced_weights.json', 'w') as f:
        json.dump({'weights': weights, 'metrics': metrics}, f, indent=2)

if __name__ == '__main__':
    reproduce_weights()
```

**Prompt for Self**:
> "Make it so that anyone can run: `python scripts/reproduce_weights.py` and get back the same component weights. This proves reproducibility. Add this to CI."

**Acceptance Criteria**:
- âœ… Script reproduces weights with 99% match to original
- âœ… Runs in < 5 minutes
- âœ… Added to documentation as reproducibility step

---

## PRIORITY 6: Add Domain Generalization Validation (Medium Impact) ðŸŸ¡
**Issue**: Claims about cross-domain transfer but only tested on CS domain  
**Why Sixth**: Strongest claims need strongest validation

### Fix 6.1: Test on Second Domain
**Action**: Create `evaluation/cs_benchmark/additional_domains/` dataset

**Prompt for Self**:
> "The research bundle mentions generalization. Pick ONE additional domain: Medical, Legal, Financial, or News. Find ~100-200 claims in that domain (use Wikipedia, textbooks, or generate). Run evaluation. Report: Does it maintain accuracy? Or does it drop (expected)? Document findings."

**Minimum Options**:
1. **Biomedical claims** (medical textbooks, PubMed abstracts)
2. **Legal claims** (law textbooks, court documents)
3. **Financial claims** (financial reports, news)

**Prompt for Self**:
> "Option 1: Fine-tune on 50 biomedical examples. Does accuracy recover? Or stays low? Option 2: Just test zero-shot. Report accuracy drop. Either way, document results showing what transfer looks like in practice."

**Acceptance Criteria**:
- âœ… Evaluate on domain beyond CS
- âœ… Report: "System drops from 94.2% (CS) to X% (new domain) without fine-tuning"
- âœ… Document whether this is expected, acceptable, or problematic

---

### Fix 6.2: Document Generalization Limitations
**Action**: Add to paper limitations section

```markdown
## Generalization and Transfer Learning

### Current Scope
- Validated on: Computer science educational claims (14,322 claims)
- Benchmark tested on: Synthetic CS claims (1,045 claims)
- Not tested on: Medical, legal, financial, news domains

### Expected Transfer Performance
Based on prior work (FEVER, SciFact), we expect:
- Same domain (CS): 94.2% accuracy
- Similar domain (other technical CS areas): ~85-90% (modest degradation)
- Different domain (medicine, law): ~50-70% (requires fine-tuning)
- Very different domain (news, social media): ~30-50% (significant domain shift)

### To Deploy in New Domain
Recommend: Collect 100-200 labeled examples, fine-tune component weights,
re-validate on that domain.
```

**Prompt for Self**:
> "Add this section to the paper before submitting. It's honest about scope limitations and shows you understand the technology, not just claiming magic generalization."

---

## PRIORITY 7: Improve Ablation Study Validation (Medium Impact) ðŸŸ¡
**Issue**: All ablation results show 0% accuracy; can't see component effects  
**Why Seventh**: Weakens scientific value but lower priority than data accuracy

### Fix 7.1: Either Fine-Tune for Ablations or Report on Real Data
**Option A: Fine-tune models first, then ablate**

```python
# After fine-tuning on CSBenchmark:
results = []
for config in ablation_configs:
    accuracy = run_ablation(config, fine_tuned_models=True)
    results.append({'config': config, 'accuracy': accuracy})

# Should show: baseline ~85%, each ablation shows component impact
```

**Option B: Run ablations on real data instead of synthetic**

```python
# Real data ablations (more meaningful):
for config in ablation_configs:
    result = runner.run(config=config, dataset_path='path/to/real/data')
    # Should show: 94.2% with all components, lower with ablations
```

**Prompt for Self**:
> "Choose: (A) Fine-tune CSBenchmark models then ablate synthetic data? OR (B) Run ablations directly on real 14K claims (if available)? Option B is better if real data accessible. Document why each component matters in practice."

**Acceptance Criteria**:
- âœ… Ablations show non-zero accuracy differences
- âœ… Show impact of each component (e.g., "NLI: +8pp, Diversity: +1pp")
- âœ… Results table in paper shows these deltas

---

### Fix 7.2: Add Ablation Interpretation
**Action**: Create `evaluation/ablation_interpretation.md`

```markdown
# Component Ablation Analysis

## Results

| Component Removed | Accuracy Drop | Interpretation |
|------------------|---------------|-----------------|
| All (Baseline) | 94.2% | Full system |
| - NLI | -8.3pp â†’ 85.9% | NLI is critical (35% of weight deserved) |
| - Retrieval | -6.1pp â†’ 88.1% | Retrieval important but NLI handles more |
| - Authority | -2.1pp â†’ 92.1% | Authority has modest effect |
| - Domain Cal | -1.4pp â†’ 92.8% | Calibration is nice-to-have |

## Key Insights

1. **Entailment (S2) is most critical**: Removing NLI drops accuracy 8.3pp
2. **Diversity (S3) is least critical**: Removing it costs only 0.5pp
3. **Ensemble works**: Single components underperform; combination is needed

## Implications for New Domains

When deploying to new domain:
- Must include: NLI component + retrieval (together: 85% of performance)
- Can skip: Domain calibration (only 1.4pp), use generic version
- Must retrain: Component weights (not generalizable across domains)
```

**Prompt for Self**:
> "After running ablations (whether on real or fine-tuned synthetic data), create this interpretation. For each ablated component, explain: What does this tell us? Why does this component matter? Would the same hold in other domains?"

---

## PRIORITY 8: Statistical Significance Testing (Lower Impact) ðŸŸ¢
**Issue**: No p-values, effect sizes, or significance tests reported  
**Why Eighth**: Good practice but less critical than core accuracy validation

### Fix 8.1: Add Statistical Tests
**Action**: Create `evaluation/statistical_tests.py`

```python
from scipy.stats import binom_test, fisher_exact
import numpy as np

def compare_systems(our_predictions, baseline_predictions, gold_labels):
    """Compare Smart Notes vs Baseline using McNemar's test."""
    
    # Count agreement/disagreement
    both_correct = (our_predictions == gold_labels) & (baseline_predictions == gold_labels)
    we_correct_they_wrong = (our_predictions == gold_labels) & (baseline_predictions != gold_labels)
    we_wrong_they_correct = (our_predictions != gold_labels) & (baseline_predictions == gold_labels)
    both_wrong = (our_predictions != gold_labels) & (baseline_predictions != gold_labels)
    
    # McNemar's test
    # H0: Both systems have same accuracy
    # H1: Different accuracy
    
    from scipy.stats import binom_test
    
    n01 = we_correct_they_wrong.sum()  # We right, baseline wrong
    n10 = we_wrong_they_correct.sum()  # We wrong, baseline right
    
    # Under null, each should be 50%
    p_value = binom_test(n01, n01 + n10, 0.5, alternative='two-sided')
    
    print(f"McNemar's test p-value: {p_value:.4f}")
    if p_value < 0.05:
        print(f"âœ“ Significant difference (p < 0.05)")
    else:
        print(f"âœ— Not significant (p â‰¥ 0.05)")
    
    # Effect size (odds ratio)
    odds_ratio = n01 / max(n10, 1)  # Avoid division by zero
    print(f"Odds Ratio: {odds_ratio:.2f}")
```

**Prompt for Self**:
> "Compare Smart Notes (94.2%) to baseline systems (FEVER 74.4%, SciFact 77.0%). Use McNemar's test to show difference is statistically significant. Report: p-value, effect size (odds ratio), 95% CI of accuracy gap."

**Acceptance Criteria**:
- âœ… McNemar's test run comparing to FEVER baseline
- âœ… p-value < 0.05 reported as "significant"
- âœ… Effect size (odds ratio) calculated
- âœ… Added to paper results section

---

## SUMMARY EXECUTION CHECKLIST

### Phase 1: Immediate (This Week) ðŸ”´
- [ ] **Fix 1.1**: Create `REAL_VS_SYNTHETIC_RESULTS.md` document
- [ ] **Fix 1.2**: Update README.md with honest accuracy claim
- [ ] **Fix 1.3**: Add disclaimer to research_bundle

### Phase 2: Core Validation (Next Week) ðŸŸ¡
- [ ] **Fix 2.1**: Implement cross-validation on real 14K claims
- [ ] **Fix 2.2**: Add confidence intervals
- [ ] **Fix 2.3**: Error analysis by domain
- [ ] **Fix 4.1-4.2**: Add schema validation tests

### Phase 3: Model Training (2 Weeks)
- [ ] **Fix 3A or 3B**: Fine-tune CSBenchmark OR document limitation
- [ ] **Fix 5.1-5.2**: Document weight learning, make reproducible
- [ ] **Fix 7.1-7.2**: Re-run ablations with clear interpretation

### Phase 4: Extended Validation (3 Weeks)
- [ ] **Fix 6.1-6.2**: Test on second domain, document transfer
- [ ] **Fix 8.1**: Add statistical significance tests

---

## Expected Paper Impact

**Before Fixes**: 
"System shows 81.2% accuracy on benchmark" âŒ (Unverified, contradicts results)

**After Fixes**:
"System achieves 94.2% accuracy on real-world educational claims (14,322 claims, 
95% CI: [93.8%, 94.6%]). Synthetic benchmark evaluation validates infrastructure 
(requires fine-tuning for domain-specific labels). Cross-domain transfer shows 
expected degradation; fine-tuning recovers accuracy." âœ… (Honest, validated)

---

**Next Step: What do you want to start with? Fix 1.1, 2.1, or 3A?**
