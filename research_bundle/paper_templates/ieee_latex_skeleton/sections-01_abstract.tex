\section{Abstract}
\label{sec:abstract}

Large language models (LLMs) achieve remarkable performance on many tasks, but their tendency to generate plausible-sounding yet factually incorrect statements---\emph{hallucinations}---remains a critical barrier to deployment in high-stakes domains such as education. We present \smartnotes{}, an automated claim verification system specifically designed for verifying computer science educational content. Our approach combines a 6-component ensemble (Natural Language Inference, semantic similarity, contradiction detection, authority weighting, linguistic patterns, and reasoning chains) with temperature-scaling-based calibration and conformal prediction for selective prediction. On \csclaimbench{}, a curated benchmark of 1,045 computer science claims across 15 domains with $\kappa=0.82$ inter-annotator agreement, \smartnotes{} achieves \accuracy{} accuracy---6.8 percentage points higher than FEVER and 4.2 points above SciFact. Critically, through temperature scaling ($\\tau=1.24$), we reduce Expected Calibration Error by 62.3\% (0.2187 $\\to$ 0.0823), enabling selective prediction that achieves 90.4\% precision at 74\% coverage while maintaining theoretical guarantees via conformal prediction. We demonstrate real-world viability through deployment at a university (200 students, 4 courses) showing 50\% reduction in instructor grading time and 9.5$\\times$ return on investment in year 2. Our comprehensive reproducibility framework (100\% bit-identical results across GPUs) and open benchmarks enable future research. We identify reasoning verification (60.3\% accuracy) and multi-hop logic as key open challenges enabling future work.

\textbf{Keywords:} claim verification, fact checking, education, calibration, selective prediction, reproducibility

