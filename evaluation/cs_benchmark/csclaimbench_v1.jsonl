{"doc_id":"algo_001","domain_topic":"Algorithms","source_text":"Breadth-First Search (BFS) is a graph traversal algorithm that explores vertices level by level. It uses a queue data structure to maintain the order of exploration. BFS guarantees finding the shortest path in unweighted graphs. The time complexity is O(V + E) where V is vertices and E is edges.","claim":"BFS has O(V + E) time complexity","gold_label":"ENTAIL","evidence_span":{"start":196,"end":249}}
{"doc_id":"algo_002","domain_topic":"Algorithms","source_text":"Breadth-First Search (BFS) is a graph traversal algorithm that explores vertices level by level. It uses a queue data structure to maintain the order of exploration. BFS guarantees finding the shortest path in unweighted graphs. The time complexity is O(V + E) where V is vertices and E is edges.","claim":"BFS uses a stack data structure for traversal","gold_label":"CONTRADICT","evidence_span":{"start":100,"end":163}}
{"doc_id":"algo_003","domain_topic":"Algorithms","source_text":"Breadth-First Search (BFS) is a graph traversal algorithm that explores vertices level by level. It uses a queue data structure to maintain the order of exploration. BFS guarantees finding the shortest path in unweighted graphs. The time complexity is O(V + E) where V is vertices and E is edges.","claim":"BFS can detect cycles in directed graphs","gold_label":"NEUTRAL"}
{"doc_id":"algo_004","domain_topic":"Algorithms","source_text":"Depth-First Search (DFS) is a graph traversal that explores as far as possible along each branch before backtracking. DFS can be implemented using recursion or an explicit stack. The time complexity is O(V + E). DFS is commonly used for topological sorting and detecting cycles.","claim":"DFS has O(V + E) time complexity","gold_label":"ENTAIL","evidence_span":{"start":161,"end":192}}
{"doc_id":"algo_005","domain_topic":"Algorithms","source_text":"Depth-First Search (DFS) is a graph traversal that explores as far as possible along each branch before backtracking. DFS can be implemented using recursion or an explicit stack. The time complexity is O(V + E). DFS is commonly used for topological sorting and detecting cycles.","claim":"DFS can be implemented using a queue","gold_label":"CONTRADICT","evidence_span":{"start":119,"end":181}}
{"doc_id":"algo_006","domain_topic":"Algorithms","source_text":"Depth-First Search (DFS) is a graph traversal that explores as far as possible along each branch before backtracking. DFS can be implemented using recursion or an explicit stack. The time complexity is O(V + E). DFS is commonly used for topological sorting and detecting cycles.","claim":"DFS guarantees the shortest path in unweighted graphs","gold_label":"NEUTRAL"}
{"doc_id":"algo_007","domain_topic":"Algorithms","source_text":"Quicksort is a divide-and-conquer sorting algorithm. It selects a pivot element and partitions the array around it. Average case time complexity is O(n log n), but worst case is O(n^2) when the pivot selection is poor. Quicksort is not stable but sorts in-place with O(log n) space for recursion stack.","claim":"Quicksort has O(n log n) average time complexity","gold_label":"ENTAIL","evidence_span":{"start":107,"end":153}}
{"doc_id":"algo_008","domain_topic":"Algorithms","source_text":"Quicksort is a divide-and-conquer sorting algorithm. It selects a pivot element and partitions the array around it. Average case time complexity is O(n log n), but worst case is O(n^2) when the pivot selection is poor. Quicksort is not stable but sorts in-place with O(log n) space for recursion stack.","claim":"Quicksort has O(n^2) best case time complexity","gold_label":"CONTRADICT","evidence_span":{"start":107,"end":153}}
{"doc_id":"algo_009","domain_topic":"Algorithms","source_text":"Quicksort is a divide-and-conquer sorting algorithm. It selects a pivot element and partitions the array around it. Average case time complexity is O(n log n), but worst case is O(n^2) when the pivot selection is poor. Quicksort is not stable but sorts in-place with O(log n) space for recursion stack.","claim":"Quicksort requires O(n) auxiliary space","gold_label":"CONTRADICT","evidence_span":{"start":248,"end":302}}
{"doc_id":"algo_010","domain_topic":"Algorithms","source_text":"Quicksort is a divide-and-conquer sorting algorithm. It selects a pivot element and partitions the array around it. Average case time complexity is O(n log n), but worst case is O(n^2) when the pivot selection is poor. Quicksort is not stable but sorts in-place with O(log n) space for recursion stack.","claim":"Quicksort can be parallelized effectively","gold_label":"NEUTRAL"}
{"doc_id":"algo_011","domain_topic":"Algorithms","source_text":"Mergesort is a stable divide-and-conquer algorithm that divides the array into halves, recursively sorts them, and merges the sorted halves. It has guaranteed O(n log n) time complexity in all cases. However, it requires O(n) auxiliary space for merging.","claim":"Mergesort has O(n log n) worst-case time complexity","gold_label":"ENTAIL","evidence_span":{"start":130,"end":186}}
{"doc_id":"algo_012","domain_topic":"Algorithms","source_text":"Mergesort is a stable divide-and-conquer algorithm that divides the array into halves, recursively sorts them, and merges the sorted halves. It has guaranteed O(n log n) time complexity in all cases. However, it requires O(n) auxiliary space for merging.","claim":"Mergesort sorts in-place","gold_label":"CONTRADICT","evidence_span":{"start":201,"end":254}}
{"doc_id":"algo_013","domain_topic":"Algorithms","source_text":"Mergesort is a stable divide-and-conquer algorithm that divides the array into halves, recursively sorts them, and merges the sorted halves. It has guaranteed O(n log n) time complexity in all cases. However, it requires O(n) auxiliary space for merging.","claim":"Mergesort is stable","gold_label":"ENTAIL","evidence_span":{"start":13,"end":29}}
{"doc_id":"algo_014","domain_topic":"Algorithms","source_text":"Mergesort is a stable divide-and-conquer algorithm that divides the array into halves, recursively sorts them, and merges the sorted halves. It has guaranteed O(n log n) time complexity in all cases. However, it requires O(n) auxiliary space for merging.","claim":"Mergesort uses a greedy approach","gold_label":"NEUTRAL"}
{"doc_id":"algo_015","domain_topic":"Algorithms","source_text":"Dijkstra\u0027s algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph. It uses a priority queue to select the vertex with minimum distance. The algorithm works correctly only with non-negative edge weights. Time complexity with binary heap is O((V + E) log V).","claim":"Dijkstra\u0027s algorithm requires non-negative edge weights","gold_label":"ENTAIL","evidence_span":{"start":181,"end":247}}
{"doc_id":"algo_016","domain_topic":"Algorithms","source_text":"Dijkstra\u0027s algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph. It uses a priority queue to select the vertex with minimum distance. The algorithm works correctly only with non-negative edge weights. Time complexity with binary heap is O((V + E) log V).","claim":"Dijkstra\u0027s algorithm can handle negative edge weights","gold_label":"CONTRADICT","evidence_span":{"start":181,"end":247}}
{"doc_id":"algo_017","domain_topic":"Algorithms","source_text":"Dijkstra\u0027s algorithm finds the shortest path from a source vertex to all other vertices in a weighted graph. It uses a priority queue to select the vertex with minimum distance. The algorithm works correctly only with non-negative edge weights. Time complexity with binary heap is O((V + E) log V).","claim":"Dijkstra\u0027s algorithm uses dynamic programming","gold_label":"NEUTRAL"}
{"doc_id":"algo_018","domain_topic":"Algorithms","source_text":"Bellman-Ford algorithm computes shortest paths from a single source vertex to all other vertices. Unlike Dijkstra, it can handle negative edge weights and detect negative cycles. The time complexity is O(VE). It relaxes all edges V-1 times.","claim":"Bellman-Ford can detect negative cycles","gold_label":"ENTAIL","evidence_span":{"start":112,"end":178}}
{"doc_id":"algo_019","domain_topic":"Algorithms","source_text":"Bellman-Ford algorithm computes shortest paths from a single source vertex to all other vertices. Unlike Dijkstra, it can handle negative edge weights and detect negative cycles. The time complexity is O(VE). It relaxes all edges V-1 times.","claim":"Bellman-Ford has O(V^2) time complexity","gold_label":"CONTRADICT","evidence_span":{"start":179,"end":208}}
{"doc_id":"algo_020","domain_topic":"Algorithms","source_text":"Bellman-Ford algorithm computes shortest paths from a single source vertex to all other vertices. Unlike Dijkstra, it can handle negative edge weights and detect negative cycles. The time complexity is O(VE). It relaxes all edges V-1 times.","claim":"Bellman-Ford uses a priority queue","gold_label":"NEUTRAL"}
{"doc_id":"algo_021","domain_topic":"Algorithms","source_text":"Binary search is an efficient algorithm for finding an element in a sorted array. It repeatedly divides the search interval in half. Time complexity is O(log n). Binary search requires the array to be sorted beforehand.","claim":"Binary search has O(log n) time complexity","gold_label":"ENTAIL","evidence_span":{"start":143,"end":162}}
{"doc_id":"algo_022","domain_topic":"Algorithms","source_text":"Binary search is an efficient algorithm for finding an element in a sorted array. It repeatedly divides the search interval in half. Time complexity is O(log n). Binary search requires the array to be sorted beforehand.","claim":"Binary search works on unsorted arrays","gold_label":"CONTRADICT","evidence_span":{"start":163,"end":219}}
{"doc_id":"algo_023","domain_topic":"Algorithms","source_text":"Binary search is an efficient algorithm for finding an element in a sorted array. It repeatedly divides the search interval in half. Time complexity is O(log n). Binary search requires the array to be sorted beforehand.","claim":"Binary search can find all occurrences of duplicates","gold_label":"NEUTRAL"}
{"doc_id":"algo_024","domain_topic":"Algorithms","source_text":"Heapsort uses a binary heap data structure to sort elements. It first builds a max-heap from the input array, then repeatedly extracts the maximum element. Time complexity is O(n log n) in all cases and it sorts in-place.","claim":"Heapsort has O(n log n) time complexity","gold_label":"ENTAIL","evidence_span":{"start":157,"end":195}}
{"doc_id":"algo_025","domain_topic":"Algorithms","source_text":"Heapsort uses a binary heap data structure to sort elements. It first builds a max-heap from the input array, then repeatedly extracts the maximum element. Time complexity is O(n log n) in all cases and it sorts in-place.","claim":"Heapsort is stable","gold_label":"NEUTRAL"}
{"doc_id":"algo_026","domain_topic":"Algorithms","source_text":"Counting sort is a non-comparison based sorting algorithm suitable for sorting integers within a limited range. It counts occurrences of each value and uses cumulative counts to place elements. Time complexity is O(n + k) where k is the range of input values.","claim":"Counting sort has O(n + k) time complexity","gold_label":"ENTAIL","evidence_span":{"start":193,"end":259}}
{"doc_id":"algo_027","domain_topic":"Algorithms","source_text":"Counting sort is a non-comparison based sorting algorithm suitable for sorting integers within a limited range. It counts occurrences of each value and uses cumulative counts to place elements. Time complexity is O(n + k) where k is the range of input values.","claim":"Counting sort is a comparison-based algorithm","gold_label":"CONTRADICT","evidence_span":{"start":18,"end":46}}
{"doc_id":"algo_028","domain_topic":"Algorithms","source_text":"Radix sort processes digits from least significant to most significant (or vice versa). It uses a stable sorting algorithm like counting sort as a subroutine. Time complexity is O(d(n + k)) where d is the number of digits.","claim":"Radix sort has time complexity O(d(n + k))","gold_label":"ENTAIL","evidence_span":{"start":159,"end":221}}
{"doc_id":"algo_029","domain_topic":"Algorithms","source_text":"Radix sort processes digits from least significant to most significant (or vice versa). It uses a stable sorting algorithm like counting sort as a subroutine. Time complexity is O(d(n + k)) where d is the number of digits.","claim":"Radix sort processes digits from most significant to least significant only","gold_label":"CONTRADICT","evidence_span":{"start":18,"end":86}}
{"doc_id":"algo_030","domain_topic":"Algorithms","source_text":"Dynamic programming solves complex problems by breaking them down into simpler subproblems and storing their solutions. It works when the problem has optimal substructure and overlapping subproblems. Classic examples include Fibonacci, knapsack, and longest common subsequence.","claim":"Dynamic programming requires optimal substructure","gold_label":"ENTAIL","evidence_span":{"start":120,"end":211}}
{"doc_id":"algo_031","domain_topic":"Algorithms","source_text":"Dynamic programming solves complex problems by breaking them down into simpler subproblems and storing their solutions. It works when the problem has optimal substructure and overlapping subproblems. Classic examples include Fibonacci, knapsack, and longest common subsequence.","claim":"Dynamic programming uses a divide-and-conquer approach without memoization","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":119}}
{"doc_id":"algo_032","domain_topic":"Algorithms","source_text":"Greedy algorithms make locally optimal choices at each step hoping to find a global optimum. They don\u0027t always produce optimal solutions. Examples include Huffman coding and Prim\u0027s algorithm for MST.","claim":"Greedy algorithms always find the optimal solution","gold_label":"CONTRADICT","evidence_span":{"start":93,"end":138}}
{"doc_id":"algo_033","domain_topic":"Algorithms","source_text":"Kruskal\u0027s algorithm finds a minimum spanning tree by sorting edges by weight and adding edges that don\u0027t create cycles. It uses a union-find data structure. Time complexity is O(E log E).","claim":"Kruskal\u0027s algorithm uses union-find","gold_label":"ENTAIL","evidence_span":{"start":120,"end":156}}
{"doc_id":"algo_034","domain_topic":"Algorithms","source_text":"Prim\u0027s algorithm grows a minimum spanning tree by starting from an arbitrary vertex and repeatedly adding the minimum weight edge that connects the tree to a new vertex. With binary heap, time complexity is O((V + E) log V).","claim":"Prim\u0027s algorithm starts from an arbitrary vertex","gold_label":"ENTAIL","evidence_span":{"start":35,"end":88}}
{"doc_id":"algo_035","domain_topic":"Algorithms","source_text":"Topological sorting produces a linear ordering of vertices in a directed acyclic graph (DAG) such that for every edge u â†’ v, u comes before v. It can be computed using DFS.","claim":"Topological sorting works on any directed graph","gold_label":"CONTRADICT","evidence_span":{"start":61,"end":91}}
{"doc_id":"ds_001","domain_topic":"DataStructures","source_text":"A stack is a Last-In-First-Out (LIFO) data structure. Elements are added and removed from the same end called the top. The main operations are push (add) and pop (remove), both taking O(1) time. Stacks are used for function call management and expression evaluation.","claim":"Stack operations push and pop take O(1) time","gold_label":"ENTAIL","evidence_span":{"start":145,"end":195}}
{"doc_id":"ds_002","domain_topic":"DataStructures","source_text":"A stack is a Last-In-First-Out (LIFO) data structure. Elements are added and removed from the same end called the top. The main operations are push (add) and pop (remove), both taking O(1) time. Stacks are used for function call management and expression evaluation.","claim":"Stack is a First-In-First-Out data structure","gold_label":"CONTRADICT","evidence_span":{"start":11,"end":53}}
{"doc_id":"ds_003","domain_topic":"DataStructures","source_text":"A stack is a Last-In-First-Out (LIFO) data structure. Elements are added and removed from the same end called the top. The main operations are push (add) and pop (remove), both taking O(1) time. Stacks are used for function call management and expression evaluation.","claim":"Stacks can be implemented using linked lists","gold_label":"NEUTRAL"}
{"doc_id":"ds_004","domain_topic":"DataStructures","source_text":"A queue is a First-In-First-Out (FIFO) data structure. Elements are added at the rear and removed from the front. Basic operations enqueue and dequeue both take O(1) time. Queues are used in BFS and scheduling algorithms.","claim":"Queue follows FIFO ordering","gold_label":"ENTAIL","evidence_span":{"start":11,"end":54}}
{"doc_id":"ds_005","domain_topic":"DataStructures","source_text":"A queue is a First-In-First-Out (FIFO) data structure. Elements are added at the rear and removed from the front. Basic operations enqueue and dequeue both take O(1) time. Queues are used in BFS and scheduling algorithms.","claim":"Elements are removed from the rear of a queue","gold_label":"CONTRADICT","evidence_span":{"start":55,"end":113}}
{"doc_id":"ds_006","domain_topic":"DataStructures","source_text":"A queue is a First-In-First-Out (FIFO) data structure. Elements are added at the rear and removed from the front. Basic operations enqueue and dequeue both take O(1) time. Queues are used in BFS and scheduling algorithms.","claim":"Priority queues maintain elements in sorted order","gold_label":"NEUTRAL"}
{"doc_id":"ds_007","domain_topic":"DataStructures","source_text":"A binary search tree (BST) is a binary tree where each node\u0027s left subtree contains only values less than the node, and the right subtree contains only values greater than the node. Search, insert, and delete operations take O(h) time where h is the height. In a balanced BST, h = log n.","claim":"BST operations take O(h) time complexity","gold_label":"ENTAIL","evidence_span":{"start":181,"end":249}}
{"doc_id":"ds_008","domain_topic":"DataStructures","source_text":"A binary search tree (BST) is a binary tree where each node\u0027s left subtree contains only values less than the node, and the right subtree contains only values greater than the node. Search, insert, and delete operations take O(h) time where h is the height. In a balanced BST, h = log n.","claim":"BST left subtree contains values greater than the node","gold_label":"CONTRADICT","evidence_span":{"start":58,"end":180}}
{"doc_id":"ds_009","domain_topic":"DataStructures","source_text":"A binary search tree (BST) is a binary tree where each node\u0027s left subtree contains only values less than the node, and the right subtree contains only values greater than the node. Search, insert, and delete operations take O(h) time where h is the height. In a balanced BST, h = log n.","claim":"BST maintains elements in sorted order during inorder traversal","gold_label":"NEUTRAL"}
{"doc_id":"ds_010","domain_topic":"DataStructures","source_text":"A binary heap is a complete binary tree that satisfies the heap property. In a max-heap, each parent node is greater than or equal to its children. In a min-heap, each parent is less than or equal to its children. Heaps support insert and extract-min/max in O(log n) time.","claim":"Heap insert takes O(log n) time","gold_label":"ENTAIL","evidence_span":{"start":215,"end":272}}
{"doc_id":"ds_011","domain_topic":"DataStructures","source_text":"A binary heap is a complete binary tree that satisfies the heap property. In a max-heap, each parent node is greater than or equal to its children. In a min-heap, each parent is less than or equal to its children. Heaps support insert and extract-min/max in O(log n) time.","claim":"In a max-heap, each parent is less than its children","gold_label":"CONTRADICT","evidence_span":{"start":74,"end":147}}
{"doc_id":"ds_012","domain_topic":"DataStructures","source_text":"A binary heap is a complete binary tree that satisfies the heap property. In a max-heap, each parent node is greater than or equal to its children. In a min-heap, each parent is less than or equal to its children. Heaps support insert and extract-min/max in O(log n) time.","claim":"Heaps can be used to implement priority queues","gold_label":"NEUTRAL"}
{"doc_id":"ds_013","domain_topic":"DataStructures","source_text":"Hash tables store key-value pairs and provide average O(1) time for insert, delete, and lookup operations. They use a hash function to map keys to array indices. Collisions are handled through chaining or open addressing.","claim":"Hash tables provide O(1) average lookup time","gold_label":"ENTAIL","evidence_span":{"start":43,"end":107}}
{"doc_id":"ds_014","domain_topic":"DataStructures","source_text":"Hash tables store key-value pairs and provide average O(1) time for insert, delete, and lookup operations. They use a hash function to map keys to array indices. Collisions are handled through chaining or open addressing.","claim":"Hash tables guarantee O(1) worst-case lookup time","gold_label":"CONTRADICT","evidence_span":{"start":43,"end":107}}
{"doc_id":"ds_015","domain_topic":"DataStructures","source_text":"Hash tables store key-value pairs and provide average O(1) time for insert, delete, and lookup operations. They use a hash function to map keys to array indices. Collisions are handled through chaining or open addressing.","claim":"Hash tables maintain keys in sorted order","gold_label":"NEUTRAL"}
{"doc_id":"ds_016","domain_topic":"DataStructures","source_text":"AVL trees are self-balancing binary search trees where the height difference between left and right subtrees is at most 1. Rotations are performed during insertions and deletions to maintain balance. All operations take O(log n) time.","claim":"AVL trees maintain height difference of at most 1","gold_label":"ENTAIL","evidence_span":{"start":58,"end":122}}
{"doc_id":"ds_017","domain_topic":"DataStructures","source_text":"AVL trees are self-balancing binary search trees where the height difference between left and right subtrees is at most 1. Rotations are performed during insertions and deletions to maintain balance. All operations take O(log n) time.","claim":"AVL tree operations take O(1) time","gold_label":"CONTRADICT","evidence_span":{"start":200,"end":233}}
{"doc_id":"ds_018","domain_topic":"DataStructures","source_text":"Red-Black trees are self-balancing BSTs with an extra color bit per node (red or black). They guarantee O(log n) time for search, insert, and delete operations. Red-Black trees have less strict balancing than AVL trees.","claim":"Red-Black trees guarantee O(log n) operations","gold_label":"ENTAIL","evidence_span":{"start":95,"end":161}}
{"doc_id":"ds_019","domain_topic":"DataStructures","source_text":"A trie (prefix tree) is a tree structure for storing strings where each node represents a character. It enables efficient prefix searches and autocomplete functionality. Search time is O(m) where m is the length of the key.","claim":"Trie search time is O(m) where m is key length","gold_label":"ENTAIL","evidence_span":{"start":168,"end":222}}
{"doc_id":"ds_020","domain_topic":"DataStructures","source_text":"A trie (prefix tree) is a tree structure for storing strings where each node represents a character. It enables efficient prefix searches and autocomplete functionality. Search time is O(m) where m is the length of the key.","claim":"Trie search is independent of key length","gold_label":"CONTRADICT","evidence_span":{"start":168,"end":222}}
{"doc_id":"ds_021","domain_topic":"DataStructures","source_text":"A disjoint set (union-find) data structure maintains a collection of disjoint sets. It supports union and find operations efficiently. With path compression and union by rank, operations take nearly O(1) amortized time.","claim":"Union-find operations take nearly O(1) amortized time","gold_label":"ENTAIL","evidence_span":{"start":135,"end":217}}
{"doc_id":"ds_022","domain_topic":"DataStructures","source_text":"A disjoint set (union-find) data structure maintains a collection of disjoint sets. It supports union and find operations efficiently. With path compression and union by rank, operations take nearly O(1) amortized time.","claim":"Union-find maintains overlapping sets","gold_label":"CONTRADICT","evidence_span":{"start":46,"end":83}}
{"doc_id":"ds_023","domain_topic":"DataStructures","source_text":"A segment tree is a binary tree used for storing intervals or segments. It allows querying which segments contain a given point efficiently. Construction takes O(n) time and queries take O(log n) time.","claim":"Segment tree queries take O(log n) time","gold_label":"ENTAIL","evidence_span":{"start":161,"end":200}}
{"doc_id":"ds_024","domain_topic":"DataStructures","source_text":"Bloom filters are probabilistic data structures for testing set membership. They can have false positives but never false negatives. Space-efficient compared to hash tables.","claim":"Bloom filters can have false positives","gold_label":"ENTAIL","evidence_span":{"start":76,"end":130}}
{"doc_id":"ds_025","domain_topic":"DataStructures","source_text":"Bloom filters are probabilistic data structures for testing set membership. They can have false positives but never false negatives. Space-efficient compared to hash tables.","claim":"Bloom filters can have false negatives","gold_label":"CONTRADICT","evidence_span":{"start":76,"end":130}}
{"doc_id":"os_001","domain_topic":"OS","source_text":"Process scheduling determines which process runs on the CPU. Round-robin scheduling allocates a fixed time quantum to each process in circular order. It ensures fairness but may have high context switch overhead. Time quantum selection is critical for performance.","claim":"Round-robin uses a fixed time quantum","gold_label":"ENTAIL","evidence_span":{"start":60,"end":149}}
{"doc_id":"os_002","domain_topic":"OS","source_text":"Process scheduling determines which process runs on the CPU. Round-robin scheduling allocates a fixed time quantum to each process in circular order. It ensures fairness but may have high context switch overhead. Time quantum selection is critical for performance.","claim":"Round-robin guarantees shortest average waiting time","gold_label":"NEUTRAL"}
{"doc_id":"os_003","domain_topic":"OS","source_text":"Process scheduling determines which process runs on the CPU. Round-robin scheduling allocates a fixed time quantum to each process in circular order. It ensures fairness but may have high context switch overhead. Time quantum selection is critical for performance.","claim":"Round-robin allocates time based on process priority","gold_label":"CONTRADICT","evidence_span":{"start":60,"end":149}}
{"doc_id":"os_004","domain_topic":"OS","source_text":"Shortest Job First (SJF) scheduling selects the process with the smallest execution time. It minimizes average waiting time but can cause starvation of long processes. SJF is optimal for minimizing average waiting time but requires knowing execution times in advance.","claim":"SJF minimizes average waiting time","gold_label":"ENTAIL","evidence_span":{"start":90,"end":138}}
{"doc_id":"os_005","domain_topic":"OS","source_text":"Shortest Job First (SJF) scheduling selects the process with the smallest execution time. It minimizes average waiting time but can cause starvation of long processes. SJF is optimal for minimizing average waiting time but requires knowing execution times in advance.","claim":"SJF can cause starvation of long processes","gold_label":"ENTAIL","evidence_span":{"start":114,"end":166}}
{"doc_id":"os_006","domain_topic":"OS","source_text":"Shortest Job First (SJF) scheduling selects the process with the smallest execution time. It minimizes average waiting time but can cause starvation of long processes. SJF is optimal for minimizing average waiting time but requires knowing execution times in advance.","claim":"SJF guarantees no process starvation","gold_label":"CONTRADICT","evidence_span":{"start":114,"end":166}}
{"doc_id":"os_007","domain_topic":"OS","source_text":"Deadlock occurs when processes are waiting for resources held by each other forming a cycle. Four necessary conditions are: mutual exclusion, hold and wait, no preemption, and circular wait. Breaking any one condition prevents deadlock.","claim":"Deadlock requires four necessary conditions","gold_label":"ENTAIL","evidence_span":{"start":92,"end":189}}
{"doc_id":"os_008","domain_topic":"OS","source_text":"Deadlock occurs when processes are waiting for resources held by each other forming a cycle. Four necessary conditions are: mutual exclusion, hold and wait, no preemption, and circular wait. Breaking any one condition prevents deadlock.","claim":"Breaking all four conditions is required to prevent deadlock","gold_label":"CONTRADICT","evidence_span":{"start":191,"end":236}}
{"doc_id":"os_009","domain_topic":"OS","source_text":"Deadlock occurs when processes are waiting for resources held by each other forming a cycle. Four necessary conditions are: mutual exclusion, hold and wait, no preemption, and circular wait. Breaking any one condition prevents deadlock.","claim":"Deadlock detection algorithms use resource allocation graphs","gold_label":"NEUTRAL"}
{"doc_id":"os_010","domain_topic":"OS","source_text":"Paging divides physical memory into fixed-size blocks called frames and logical memory into pages of the same size. It eliminates external fragmentation but may cause internal fragmentation. A page table maps logical to physical addresses.","claim":"Paging eliminates external fragmentation","gold_label":"ENTAIL","evidence_span":{"start":117,"end":188}}
{"doc_id":"os_011","domain_topic":"OS","source_text":"Paging divides physical memory into fixed-size blocks called frames and logical memory into pages of the same size. It eliminates external fragmentation but may cause internal fragmentation. A page table maps logical to physical addresses.","claim":"Paging eliminates internal fragmentation","gold_label":"CONTRADICT","evidence_span":{"start":117,"end":188}}
{"doc_id":"os_012","domain_topic":"OS","source_text":"Paging divides physical memory into fixed-size blocks called frames and logical memory into pages of the same size. It eliminates external fragmentation but may cause internal fragmentation. A page table maps logical to physical addresses.","claim":"Paging uses variable-size memory blocks","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":116}}
{"doc_id":"os_013","domain_topic":"OS","source_text":"Virtual memory allows execution of processes that may not be completely in physical memory. It uses demand paging to load pages only when needed. Benefits include running larger programs and better memory utilization. Page faults occur when accessing pages not in memory.","claim":"Virtual memory uses demand paging","gold_label":"ENTAIL","evidence_span":{"start":92,"end":147}}
{"doc_id":"os_014","domain_topic":"OS","source_text":"Virtual memory allows execution of processes that may not be completely in physical memory. It uses demand paging to load pages only when needed. Benefits include running larger programs and better memory utilization. Page faults occur when accessing pages not in memory.","claim":"Virtual memory loads all pages into memory at process start","gold_label":"CONTRADICT","evidence_span":{"start":92,"end":147}}
{"doc_id":"os_015","domain_topic":"OS","source_text":"The Least Recently Used (LRU) page replacement algorithm replaces the page that has not been used for the longest time. It approximates optimal page replacement and performs well in practice. Implementation requires tracking page access times.","claim":"LRU replaces the least recently used page","gold_label":"ENTAIL","evidence_span":{"start":55,"end":119}}
{"doc_id":"os_016","domain_topic":"OS","source_text":"The Least Recently Used (LRU) page replacement algorithm replaces the page that has not been used for the longest time. It approximates optimal page replacement and performs well in practice. Implementation requires tracking page access times.","claim":"LRU is the optimal page replacement algorithm","gold_label":"CONTRADICT","evidence_span":{"start":120,"end":191}}
{"doc_id":"os_017","domain_topic":"OS","source_text":"Semaphores are synchronization primitives with an integer value and two atomic operations: wait (P) and signal (V). Binary semaphores have values 0 or 1, while counting semaphores can have any non-negative value. They prevent race conditions.","claim":"Binary semaphores have values 0 or 1","gold_label":"ENTAIL","evidence_span":{"start":113,"end":165}}
{"doc_id":"os_018","domain_topic":"OS","source_text":"Semaphores are synchronization primitives with an integer value and two atomic operations: wait (P) and signal (V). Binary semaphores have values 0 or 1, while counting semaphores can have any non-negative value. They prevent race conditions.","claim":"Counting semaphores are limited to binary values","gold_label":"CONTRADICT","evidence_span":{"start":113,"end":211}}
{"doc_id":"os_019","domain_topic":"OS","source_text":"Monitors are high-level synchronization constructs that provide mutual exclusion. They encapsulate shared data and procedures that operate on the data. Only one process can be active inside a monitor at a time.","claim":"Only one process can be active in a monitor at a time","gold_label":"ENTAIL","evidence_span":{"start":149,"end":209}}
{"doc_id":"os_020","domain_topic":"OS","source_text":"The producer-consumer problem involves processes that produce and consume items from a shared buffer. Synchronization is needed to prevent buffer overflow and underflow. Solutions use semaphores or monitors.","claim":"Producer-consumer problem requires synchronization","gold_label":"ENTAIL","evidence_span":{"start":102,"end":169}}
{"doc_id":"os_021","domain_topic":"OS","source_text":"Context switching is the process of saving the state of a currently running process and loading the saved state of another process. It includes saving registers, program counter, and other context. High context switch frequency increases overhead.","claim":"Context switching saves registers and program counter","gold_label":"ENTAIL","evidence_span":{"start":131,"end":198}}
{"doc_id":"os_022","domain_topic":"OS","source_text":"Threads are lightweight processes that share the same address space. They have separate stacks and registers but share code, data, and files. Thread creation is faster than process creation.","claim":"Threads share the same address space","gold_label":"ENTAIL","evidence_span":{"start":30,"end":68}}
{"doc_id":"os_023","domain_topic":"OS","source_text":"Threads are lightweight processes that share the same address space. They have separate stacks and registers but share code, data, and files. Thread creation is faster than process creation.","claim":"Threads have separate code segments","gold_label":"CONTRADICT","evidence_span":{"start":70,"end":142}}
{"doc_id":"os_024","domain_topic":"OS","source_text":"Priority inversion occurs when a high-priority task waits for a resource held by a low-priority task. Priority inheritance is one solution where the low-priority task temporarily inherits the high priority.","claim":"Priority inversion involves high-priority tasks waiting","gold_label":"ENTAIL","evidence_span":{"start":0,"end":100}}
{"doc_id":"os_025","domain_topic":"OS","source_text":"File systems organize and store files on storage devices. They manage file metadata including name, size, permissions, and timestamps. Common file systems include ext4, NTFS, and FAT32.","claim":"File systems manage file metadata","gold_label":"ENTAIL","evidence_span":{"start":57,"end":132}}
{"doc_id":"db_001","domain_topic":"DB","source_text":"ACID properties ensure reliable database transactions. Atomicity means transactions are all-or-nothing. Consistency ensures the database moves from one valid state to another. Isolation prevents concurrent transactions from interfering. Durability guarantees committed transactions persist even after system failure.","claim":"Atomicity means transactions are all-or-nothing","gold_label":"ENTAIL","evidence_span":{"start":55,"end":103}}
{"doc_id":"db_002","domain_topic":"DB","source_text":"ACID properties ensure reliable database transactions. Atomicity means transactions are all-or-nothing. Consistency ensures the database moves from one valid state to another. Isolation prevents concurrent transactions from interfering. Durability guarantees committed transactions persist even after system failure.","claim":"Durability means data is lost after system crashes","gold_label":"CONTRADICT","evidence_span":{"start":238,"end":315}}
{"doc_id":"db_003","domain_topic":"DB","source_text":"ACID properties ensure reliable database transactions. Atomicity means transactions are all-or-nothing. Consistency ensures the database moves from one valid state to another. Isolation prevents concurrent transactions from interfering. Durability guarantees committed transactions persist even after system failure.","claim":"ACID includes availability as a property","gold_label":"NEUTRAL"}
{"doc_id":"db_004","domain_topic":"DB","source_text":"Normalization reduces data redundancy and improves data integrity. First Normal Form (1NF) requires atomic values in each cell. Second Normal Form (2NF) requires no partial dependencies. Third Normal Form (3NF) requires no transitive dependencies.","claim":"1NF requires atomic values in each cell","gold_label":"ENTAIL","evidence_span":{"start":67,"end":125}}
{"doc_id":"db_005","domain_topic":"DB","source_text":"Normalization reduces data redundancy and improves data integrity. First Normal Form (1NF) requires atomic values in each cell. Second Normal Form (2NF) requires no partial dependencies. Third Normal Form (3NF) requires no transitive dependencies.","claim":"3NF allows transitive dependencies","gold_label":"CONTRADICT","evidence_span":{"start":187,"end":246}}
{"doc_id":"db_006","domain_topic":"DB","source_text":"Normalization reduces data redundancy and improves data integrity. First Normal Form (1NF) requires atomic values in each cell. Second Normal Form (2NF) requires no partial dependencies. Third Normal Form (3NF) requires no transitive dependencies.","claim":"Normalization always improves query performance","gold_label":"NEUTRAL"}
{"doc_id":"db_007","domain_topic":"DB","source_text":"Database indexes improve query performance by providing fast lookup paths. B-tree indexes maintain sorted order and support range queries efficiently. Hash indexes provide O(1) lookup for equality searches but don\u0027t support range queries. Indexes speed up reads but slow down writes.","claim":"B-tree indexes support range queries","gold_label":"ENTAIL","evidence_span":{"start":74,"end":162}}
{"doc_id":"db_008","domain_topic":"DB","source_text":"Database indexes improve query performance by providing fast lookup paths. B-tree indexes maintain sorted order and support range queries efficiently. Hash indexes provide O(1) lookup for equality searches but don\u0027t support range queries. Indexes speed up reads but slow down writes.","claim":"Hash indexes support range queries","gold_label":"CONTRADICT","evidence_span":{"start":151,"end":238}}
{"doc_id":"db_009","domain_topic":"DB","source_text":"Database indexes improve query performance by providing fast lookup paths. B-tree indexes maintain sorted order and support range queries efficiently. Hash indexes provide O(1) lookup for equality searches but don\u0027t support range queries. Indexes speed up reads but slow down writes.","claim":"Indexes always improve database performance","gold_label":"CONTRADICT","evidence_span":{"start":239,"end":283}}
{"doc_id":"db_010","domain_topic":"DB","source_text":"SQL joins combine rows from two or more tables. INNER JOIN returns only matching rows. LEFT JOIN returns all rows from left table and matching rows from right. RIGHT JOIN returns all rows from right table. FULL OUTER JOIN returns all rows from both tables.","claim":"INNER JOIN returns only matching rows","gold_label":"ENTAIL","evidence_span":{"start":48,"end":85}}
{"doc_id":"db_011","domain_topic":"DB","source_text":"SQL joins combine rows from two or more tables. INNER JOIN returns only matching rows. LEFT JOIN returns all rows from left table and matching rows from right. RIGHT JOIN returns all rows from right table. FULL OUTER JOIN returns all rows from both tables.","claim":"LEFT JOIN returns only matching rows from both tables","gold_label":"CONTRADICT","evidence_span":{"start":87,"end":160}}
{"doc_id":"db_012","domain_topic":"DB","source_text":"Transaction isolation levels control the visibility of changes between concurrent transactions. READ UNCOMMITTED allows dirty reads. READ COMMITTED prevents dirty reads. REPEATABLE READ prevents non-repeatable reads. SERIALIZABLE provides complete isolation.","claim":"SERIALIZABLE provides complete isolation","gold_label":"ENTAIL","evidence_span":{"start":218,"end":257}}
{"doc_id":"db_013","domain_topic":"DB","source_text":"Transaction isolation levels control the visibility of changes between concurrent transactions. READ UNCOMMITTED allows dirty reads. READ COMMITTED prevents dirty reads. REPEATABLE READ prevents non-repeatable reads. SERIALIZABLE provides complete isolation.","claim":"READ COMMITTED allows dirty reads","gold_label":"CONTRADICT","evidence_span":{"start":132,"end":168}}
{"doc_id":"db_014","domain_topic":"DB","source_text":"Two-phase locking (2PL) ensures serializability. Growing phase acquires locks, shrinking phase releases locks. Strict 2PL holds write locks until commit. Deadlocks can occur with 2PL.","claim":"2PL can cause deadlocks","gold_label":"ENTAIL","evidence_span":{"start":152,"end":182}}
{"doc_id":"db_015","domain_topic":"DB","source_text":"Two-phase locking (2PL) ensures serializability. Growing phase acquires locks, shrinking phase releases locks. Strict 2PL holds write locks until commit. Deadlocks can occur with 2PL.","claim":"2PL prevents all deadlocks","gold_label":"CONTRADICT","evidence_span":{"start":152,"end":182}}
{"doc_id":"db_016","domain_topic":"DB","source_text":"NoSQL databases provide flexible schemas and horizontal scaling. Document stores like MongoDB use JSON-like documents. Key-value stores like Redis provide simple get/put operations. Column-family stores like Cassandra organize data by columns.","claim":"MongoDB is a document store","gold_label":"ENTAIL","evidence_span":{"start":65,"end":117}}
{"doc_id":"db_017","domain_topic":"DB","source_text":"NoSQL databases provide flexible schemas and horizontal scaling. Document stores like MongoDB use JSON-like documents. Key-value stores like Redis provide simple get/put operations. Column-family stores like Cassandra organize data by columns.","claim":"Redis is a column-family store","gold_label":"CONTRADICT","evidence_span":{"start":119,"end":181}}
{"doc_id":"db_018","domain_topic":"DB","source_text":"Database sharding partitions data across multiple servers. It improves scalability but increases query complexity. Consistent hashing helps distribute data evenly.","claim":"Sharding partitions data across multiple servers","gold_label":"ENTAIL","evidence_span":{"start":0,"end":58}}
{"doc_id":"db_019","domain_topic":"DB","source_text":"Database replication copies data to multiple servers for availability and performance. Master-slave replication has one master for writes and multiple slaves for reads. Multi-master replication allows writes to multiple nodes but requires conflict resolution.","claim":"Master-slave replication has one master for writes","gold_label":"ENTAIL","evidence_span":{"start":86,"end":168}}
{"doc_id":"db_020","domain_topic":"DB","source_text":"Database replication copies data to multiple servers for availability and performance. Master-slave replication has one master for writes and multiple slaves for reads. Multi-master replication allows writes to multiple nodes but requires conflict resolution.","claim":"Multi-master allows writes to only one node","gold_label":"CONTRADICT","evidence_span":{"start":170,"end":257}}
{"doc_id":"db_021","domain_topic":"DB","source_text":"Query optimization transforms queries into efficient execution plans. Cost-based optimizers estimate the cost of different plans and choose the cheapest. Statistics about data distribution help optimizers make better decisions.","claim":"Cost-based optimizers estimate plan costs","gold_label":"ENTAIL","evidence_span":{"start":69,"end":153}}
{"doc_id":"db_022","domain_topic":"DB","source_text":"Materialized views store query results physically. They improve read performance but require storage and maintenance. Updates to base tables must be reflected in materialized views.","claim":"Materialized views improve read performance","gold_label":"ENTAIL","evidence_span":{"start":51,"end":117}}
{"doc_id":"db_023","domain_topic":"DB","source_text":"Database triggers are procedures that automatically execute in response to events. They can enforce business rules and maintain audit trails. Triggers increase coupling and can impact performance.","claim":"Triggers execute automatically in response to events","gold_label":"ENTAIL","evidence_span":{"start":0,"end":81}}
{"doc_id":"db_024","domain_topic":"DB","source_text":"Foreign keys enforce referential integrity between tables. They ensure that relationships between tables remain consistent. Cascading deletes can automatically remove related rows.","claim":"Foreign keys enforce referential integrity","gold_label":"ENTAIL","evidence_span":{"start":0,"end":60}}
{"doc_id":"db_025","domain_topic":"DB","source_text":"OLTP systems handle many short transactions focused on data entry and updates. OLAP systems handle complex analytical queries on historical data. OLTP uses normalized schemas while OLAP uses denormalized schemas like star schema.","claim":"OLTP uses normalized schemas","gold_label":"ENTAIL","evidence_span":{"start":147,"end":229}}
{"doc_id":"distributed_001","domain_topic":"Distributed","source_text":"The CAP theorem states that a distributed system can provide at most two of three guarantees: Consistency (all nodes see the same data), Availability (every request receives a response), and Partition tolerance (system works despite network partitions). In the presence of partitions, you must choose between consistency and availability.","claim":"CAP theorem allows at most two of three properties","gold_label":"ENTAIL","evidence_span":{"start":20,"end":115}}
{"doc_id":"distributed_002","domain_topic":"Distributed","source_text":"The CAP theorem states that a distributed system can provide at most two of three guarantees: Consistency (all nodes see the same data), Availability (every request receives a response), and Partition tolerance (system works despite network partitions). In the presence of partitions, you must choose between consistency and availability.","claim":"A distributed system can guarantee all three CAP properties","gold_label":"CONTRADICT","evidence_span":{"start":20,"end":115}}
{"doc_id":"distributed_003","domain_topic":"Distributed","source_text":"The CAP theorem states that a distributed system can provide at most two of three guarantees: Consistency (all nodes see the same data), Availability (every request receives a response), and Partition tolerance (system works despite network partitions). In the presence of partitions, you must choose between consistency and availability.","claim":"CAP theorem was proposed by Eric Brewer","gold_label":"NEUTRAL"}
{"doc_id":"distributed_004","domain_topic":"Distributed","source_text":"Paxos is a consensus algorithm that allows distributed systems to agree on a value even in the presence of failures. It uses a majority quorum to make progress. Paxos guarantees safety but not liveness under certain conditions.","claim":"Paxos uses a majority quorum","gold_label":"ENTAIL","evidence_span":{"start":119,"end":161}}
{"doc_id":"distributed_005","domain_topic":"Distributed","source_text":"Paxos is a consensus algorithm that allows distributed systems to agree on a value even in the presence of failures. It uses a majority quorum to make progress. Paxos guarantees safety but not liveness under certain conditions.","claim":"Paxos guarantees both safety and liveness always","gold_label":"CONTRADICT","evidence_span":{"start":162,"end":226}}
{"doc_id":"distributed_006","domain_topic":"Distributed","source_text":"Raft is a consensus algorithm designed to be more understandable than Paxos. It divides consensus into leader election, log replication, and safety. Raft ensures that committed entries are durable and will eventually be executed by all servers.","claim":"Raft divides consensus into leader election, log replication, and safety","gold_label":"ENTAIL","evidence_span":{"start":77,"end":148}}
{"doc_id":"distributed_007","domain_topic":"Distributed","source_text":"Raft is a consensus algorithm designed to be more understandable than Paxos. It divides consensus into leader election, log replication, and safety. Raft ensures that committed entries are durable and will eventually be executed by all servers.","claim":"Raft is more complex than Paxos","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":76}}
{"doc_id":"distributed_008","domain_topic":"Distributed","source_text":"Two-phase commit (2PC) is a protocol for atomic commitment across distributed systems. Phase 1 is voting where coordinator asks participants to prepare. Phase 2 is decision where coordinator commits or aborts based on votes. 2PC blocks if coordinator fails.","claim":"2PC has voting and decision phases","gold_label":"ENTAIL","evidence_span":{"start":86,"end":222}}
{"doc_id":"distributed_009","domain_topic":"Distributed","source_text":"Two-phase commit (2PC) is a protocol for atomic commitment across distributed systems. Phase 1 is voting where coordinator asks participants to prepare. Phase 2 is decision where coordinator commits or aborts based on votes. 2PC blocks if coordinator fails.","claim":"2PC never blocks during failures","gold_label":"CONTRADICT","evidence_span":{"start":223,"end":257}}
{"doc_id":"distributed_010","domain_topic":"Distributed","source_text":"Vector clocks track causality in distributed systems. Each process maintains a vector of logical timestamps. They can determine if events are concurrent or causally related. Vector clocks grow with the number of processes.","claim":"Vector clocks track causality","gold_label":"ENTAIL","evidence_span":{"start":0,"end":53}}
{"doc_id":"distributed_011","domain_topic":"Distributed","source_text":"Vector clocks track causality in distributed systems. Each process maintains a vector of logical timestamps. They can determine if events are concurrent or causally related. Vector clocks grow with the number of processes.","claim":"Vector clocks have constant size","gold_label":"CONTRADICT","evidence_span":{"start":173,"end":222}}
{"doc_id":"distributed_012","domain_topic":"Distributed","source_text":"Eventual consistency allows temporary inconsistencies in distributed systems with the guarantee that all replicas will eventually converge. It provides high availability but requires conflict resolution. DNS is an example of eventual consistency.","claim":"Eventual consistency guarantees all replicas eventually converge","gold_label":"ENTAIL","evidence_span":{"start":24,"end":138}}
{"doc_id":"distributed_013","domain_topic":"Distributed","source_text":"Eventual consistency allows temporary inconsistencies in distributed systems with the guarantee that all replicas will eventually converge. It provides high availability but requires conflict resolution. DNS is an example of eventual consistency.","claim":"Eventual consistency provides immediate consistency","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":138}}
{"doc_id":"distributed_014","domain_topic":"Distributed","source_text":"Consistent hashing distributes data across nodes in a way that minimizes reorganization when nodes are added or removed. Hash values are assigned to points on a circle. Adding or removing a node affects only its neighbors.","claim":"Consistent hashing minimizes reorganization when nodes change","gold_label":"ENTAIL","evidence_span":{"start":21,"end":119}}
{"doc_id":"distributed_015","domain_topic":"Distributed","source_text":"Consistent hashing distributes data across nodes in a way that minimizes reorganization when nodes are added or removed. Hash values are assigned to points on a circle. Adding or removing a node affects only its neighbors.","claim":"Consistent hashing requires complete data redistribution when nodes change","gold_label":"CONTRADICT","evidence_span":{"start":21,"end":119}}
{"doc_id":"distributed_016","domain_topic":"Distributed","source_text":"Byzantine fault tolerance handles arbitrary failures including malicious behavior. Byzantine agreement algorithms require at least 3f+1 nodes to tolerate f Byzantine failures. PBFT is a practical Byzantine fault tolerance algorithm.","claim":"Byzantine fault tolerance requires at least 3f+1 nodes for f failures","gold_label":"ENTAIL","evidence_span":{"start":82,"end":174}}
{"doc_id":"distributed_017","domain_topic":"Distributed","source_text":"Byzantine fault tolerance handles arbitrary failures including malicious behavior. Byzantine agreement algorithms require at least 3f+1 nodes to tolerate f Byzantine failures. PBFT is a practical Byzantine fault tolerance algorithm.","claim":"Byzantine fault tolerance requires at least 2f+1 nodes","gold_label":"CONTRADICT","evidence_span":{"start":82,"end":174}}
{"doc_id":"distributed_018","domain_topic":"Distributed","source_text":"Distributed transactions use protocols like 2PC to maintain ACID properties across multiple databases. They can suffer from high latency and reduced availability. Saga pattern provides an alternative using compensating transactions.","claim":"Distributed transactions use 2PC for ACID properties","gold_label":"ENTAIL","evidence_span":{"start":0,"end":101}}
{"doc_id":"distributed_019","domain_topic":"Distributed","source_text":"Gossip protocols spread information through a distributed system by randomly exchanging information with peers. They provide eventual consistency and are resilient to node failures. Each node periodically sends updates to a random subset of nodes.","claim":"Gossip protocols randomly exchange information","gold_label":"ENTAIL","evidence_span":{"start":62,"end":109}}
{"doc_id":"distributed_020","domain_topic":"Distributed","source_text":"Leader election algorithms ensure a single coordinator in distributed systems. Bully algorithm elects the highest ID node. Ring algorithm passes election messages in a ring. Both handle coordinator failures.","claim":"Bully algorithm elects the highest ID node","gold_label":"ENTAIL","evidence_span":{"start":79,"end":122}}
{"doc_id":"distributed_021","domain_topic":"Distributed","source_text":"MapReduce is a programming model for processing large datasets. Map phase transforms input into key-value pairs. Reduce phase aggregates values for each key. It provides automatic parallelization and fault tolerance.","claim":"MapReduce has map and reduce phases","gold_label":"ENTAIL","evidence_span":{"start":65,"end":157}}
{"doc_id":"distributed_022","domain_topic":"Distributed","source_text":"Quorum-based replication requires reading from r replicas and writing to w replicas where r + w \u003e n (n is total replicas). This ensures reads see latest writes. Quorums trade off consistency and availability.","claim":"Quorum replication requires r + w \u003e n","gold_label":"ENTAIL","evidence_span":{"start":32,"end":120}}
{"doc_id":"distributed_023","domain_topic":"Distributed","source_text":"Load balancing distributes requests across multiple servers. Round-robin sends requests to servers in rotation. Least connections sends to server with fewest active connections. Consistent hashing is used for caching.","claim":"Round-robin distributes requests in rotation","gold_label":"ENTAIL","evidence_span":{"start":61,"end":109}}
{"doc_id":"distributed_024","domain_topic":"Distributed","source_text":"Service discovery allows services to find each other in distributed systems. DNS-based discovery uses domain names. Service registries like Consul maintain service locations. Health checks ensure only healthy services are routed to.","claim":"Service registries maintain service locations","gold_label":"ENTAIL","evidence_span":{"start":117,"end":173}}
{"doc_id":"distributed_025","domain_topic":"Distributed","source_text":"Sharding partitions data horizontally across multiple databases. Each shard contains a subset of the data. Sharding improves scalability but complicates queries spanning multiple shards.","claim":"Sharding partitions data horizontally","gold_label":"ENTAIL","evidence_span":{"start":0,"end":64}}
{"doc_id":"networks_001","domain_topic":"Networks","source_text":"TCP provides reliable, ordered, connection-oriented communication. It uses three-way handshake for connection establishment (SYN, SYN-ACK, ACK). TCP guarantees delivery through acknowledgments and retransmissions. Flow control uses sliding window protocol.","claim":"TCP uses three-way handshake for connection establishment","gold_label":"ENTAIL","evidence_span":{"start":67,"end":145}}
{"doc_id":"networks_002","domain_topic":"Networks","source_text":"TCP provides reliable, ordered, connection-oriented communication. It uses three-way handshake for connection establishment (SYN, SYN-ACK, ACK). TCP guarantees delivery through acknowledgments and retransmissions. Flow control uses sliding window protocol.","claim":"TCP is connectionless","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":66}}
{"doc_id":"networks_003","domain_topic":"Networks","source_text":"TCP provides reliable, ordered, connection-oriented communication. It uses three-way handshake for connection establishment (SYN, SYN-ACK, ACK). TCP guarantees delivery through acknowledgments and retransmissions. Flow control uses sliding window protocol.","claim":"TCP uses selective repeat ARQ","gold_label":"NEUTRAL"}
{"doc_id":"networks_004","domain_topic":"Networks","source_text":"UDP is a connectionless, unreliable protocol with no delivery guarantees. It has minimal overhead and lower latency than TCP. UDP is used for streaming, DNS queries, and real-time applications where some packet loss is acceptable.","claim":"UDP is connectionless and unreliable","gold_label":"ENTAIL","evidence_span":{"start":0,"end":73}}
{"doc_id":"networks_005","domain_topic":"Networks","source_text":"UDP is a connectionless, unreliable protocol with no delivery guarantees. It has minimal overhead and lower latency than TCP. UDP is used for streaming, DNS queries, and real-time applications where some packet loss is acceptable.","claim":"UDP guarantees packet delivery","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":73}}
{"doc_id":"networks_006","domain_topic":"Networks","source_text":"UDP is a connectionless, unreliable protocol with no delivery guarantees. It has minimal overhead and lower latency than TCP. UDP is used for streaming, DNS queries, and real-time applications where some packet loss is acceptable.","claim":"UDP has lower latency than TCP","gold_label":"ENTAIL","evidence_span":{"start":75,"end":125}}
{"doc_id":"networks_007","domain_topic":"Networks","source_text":"IP addresses identify devices on a network. IPv4 uses 32-bit addresses allowing about 4 billion addresses. IPv6 uses 128-bit addresses providing virtually unlimited address space. NAT allows multiple devices to share a single public IP.","claim":"IPv4 uses 32-bit addresses","gold_label":"ENTAIL","evidence_span":{"start":44,"end":106}}
{"doc_id":"networks_008","domain_topic":"Networks","source_text":"IP addresses identify devices on a network. IPv4 uses 32-bit addresses allowing about 4 billion addresses. IPv6 uses 128-bit addresses providing virtually unlimited address space. NAT allows multiple devices to share a single public IP.","claim":"IPv6 uses 32-bit addresses","gold_label":"CONTRADICT","evidence_span":{"start":107,"end":179}}
{"doc_id":"networks_009","domain_topic":"Networks","source_text":"DNS (Domain Name System) translates domain names to IP addresses. It uses a hierarchical distributed database. DNS queries can be recursive or iterative. Caching improves DNS performance.","claim":"DNS translates domain names to IP addresses","gold_label":"ENTAIL","evidence_span":{"start":0,"end":65}}
{"doc_id":"networks_010","domain_topic":"Networks","source_text":"DNS (Domain Name System) translates domain names to IP addresses. It uses a hierarchical distributed database. DNS queries can be recursive or iterative. Caching improves DNS performance.","claim":"DNS uses a centralized database","gold_label":"CONTRADICT","evidence_span":{"start":67,"end":109}}
{"doc_id":"networks_011","domain_topic":"Networks","source_text":"HTTP is an application layer protocol for web communication. HTTP/1.1 introduced persistent connections. HTTP/2 uses binary framing and multiplexing. HTTPS adds TLS encryption for security.","claim":"HTTP/2 uses binary framing","gold_label":"ENTAIL","evidence_span":{"start":106,"end":148}}
{"doc_id":"networks_012","domain_topic":"Networks","source_text":"HTTP is an application layer protocol for web communication. HTTP/1.1 introduced persistent connections. HTTP/2 uses binary framing and multiplexing. HTTPS adds TLS encryption for security.","claim":"HTTPS provides no encryption","gold_label":"CONTRADICT","evidence_span":{"start":150,"end":188}}
{"doc_id":"networks_013","domain_topic":"Networks","source_text":"Routing protocols determine paths for data packets. Distance vector protocols like RIP use hop count. Link state protocols like OSPF maintain network topology maps. BGP is used for inter-domain routing.","claim":"OSPF is a link state protocol","gold_label":"ENTAIL","evidence_span":{"start":102,"end":163}}
{"doc_id":"networks_014","domain_topic":"Networks","source_text":"Routing protocols determine paths for data packets. Distance vector protocols like RIP use hop count. Link state protocols like OSPF maintain network topology maps. BGP is used for inter-domain routing.","claim":"RIP is a link state protocol","gold_label":"CONTRADICT","evidence_span":{"start":52,"end":101}}
{"doc_id":"networks_015","domain_topic":"Networks","source_text":"Subnetting divides IP networks into smaller subnetworks. It uses subnet masks to determine network and host portions. CIDR notation represents subnets concisely (e.g., 192.168.1.0/24).","claim":"Subnetting uses subnet masks","gold_label":"ENTAIL","evidence_span":{"start":57,"end":118}}
{"doc_id":"networks_016","domain_topic":"Networks","source_text":"Firewalls filter network traffic based on rules. Packet filtering examines headers. Stateful firewalls track connection state. Application firewalls operate at layer 7.","claim":"Stateful firewalls track connection state","gold_label":"ENTAIL","evidence_span":{"start":85,"end":127}}
{"doc_id":"networks_017","domain_topic":"Networks","source_text":"VPNs create secure tunnels over public networks. They encrypt traffic and can provide remote access to private networks. IPsec and SSL/TLS are common VPN protocols.","claim":"VPNs encrypt traffic","gold_label":"ENTAIL","evidence_span":{"start":49,"end":119}}
{"doc_id":"networks_018","domain_topic":"Networks","source_text":"CDNs (Content Delivery Networks) distribute content across geographically dispersed servers. They reduce latency by serving content from nearby servers. Caching and load balancing improve performance.","claim":"CDNs reduce latency by using nearby servers","gold_label":"ENTAIL","evidence_span":{"start":93,"end":153}}
{"doc_id":"networks_019","domain_topic":"Networks","source_text":"The OSI model has 7 layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. It provides a conceptual framework for network protocols.","claim":"OSI model has 7 layers","gold_label":"ENTAIL","evidence_span":{"start":0,"end":108}}
{"doc_id":"networks_020","domain_topic":"Networks","source_text":"The OSI model has 7 layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. It provides a conceptual framework for network protocols.","claim":"OSI model has 5 layers","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":108}}
{"doc_id":"networks_021","domain_topic":"Networks","source_text":"ARP (Address Resolution Protocol) maps IP addresses to MAC addresses in local networks. It broadcasts ARP requests to find the MAC address for an IP. ARP cache stores mappings temporarily.","claim":"ARP maps IP addresses to MAC addresses","gold_label":"ENTAIL","evidence_span":{"start":0,"end":87}}
{"doc_id":"networks_022","domain_topic":"Networks","source_text":"DHCP (Dynamic Host Configuration Protocol) automatically assigns IP addresses to devices. It uses a lease system where addresses are assigned temporarily. DHCP reduces manual configuration.","claim":"DHCP automatically assigns IP addresses","gold_label":"ENTAIL","evidence_span":{"start":0,"end":90}}
{"doc_id":"networks_023","domain_topic":"Networks","source_text":"Network congestion control prevents network overload. TCP uses congestion window and slow start algorithms. RED (Random Early Detection) drops packets proactively to signal congestion.","claim":"TCP uses congestion window for congestion control","gold_label":"ENTAIL","evidence_span":{"start":54,"end":107}}
{"doc_id":"networks_024","domain_topic":"Networks","source_text":"QoS (Quality of Service) prioritizes certain network traffic. Traffic shaping controls bandwidth usage. Packet scheduling determines transmission order. DiffServ and IntServ are QoS architectures.","claim":"QoS prioritizes network traffic","gold_label":"ENTAIL","evidence_span":{"start":0,"end":61}}
{"doc_id":"networks_025","domain_topic":"Networks","source_text":"WebSockets provide full-duplex communication over TCP. Unlike HTTP request-response, WebSockets maintain persistent connections. They\u0027re used for real-time applications like chat and live updates.","claim":"WebSockets provide full-duplex communication","gold_label":"ENTAIL","evidence_span":{"start":0,"end":54}}
{"doc_id":"compilers_001","domain_topic":"Compilers","source_text":"Lexical analysis (lexing) is the first phase of compilation. It reads source code and produces tokens. Tokens are meaningful sequences like keywords, identifiers, and operators. Regular expressions and finite automata are used for lexical analysis.","claim":"Lexical analysis produces tokens","gold_label":"ENTAIL","evidence_span":{"start":61,"end":102}}
{"doc_id":"compilers_002","domain_topic":"Compilers","source_text":"Lexical analysis (lexing) is the first phase of compilation. It reads source code and produces tokens. Tokens are meaningful sequences like keywords, identifiers, and operators. Regular expressions and finite automata are used for lexical analysis.","claim":"Lexical analysis produces abstract syntax trees","gold_label":"CONTRADICT","evidence_span":{"start":61,"end":102}}
{"doc_id":"compilers_003","domain_topic":"Compilers","source_text":"Lexical analysis (lexing) is the first phase of compilation. It reads source code and produces tokens. Tokens are meaningful sequences like keywords, identifiers, and operators. Regular expressions and finite automata are used for lexical analysis.","claim":"Lexical analysis uses regular expressions","gold_label":"ENTAIL","evidence_span":{"start":178,"end":246}}
{"doc_id":"compilers_004","domain_topic":"Compilers","source_text":"Syntax analysis (parsing) checks if tokens form valid grammatical structures. It uses context-free grammars to define syntax rules. Parsers build parse trees or abstract syntax trees (ASTs). Common parsing techniques include LL and LR parsing.","claim":"Parsing uses context-free grammars","gold_label":"ENTAIL","evidence_span":{"start":77,"end":131}}
{"doc_id":"compilers_005","domain_topic":"Compilers","source_text":"Syntax analysis (parsing) checks if tokens form valid grammatical structures. It uses context-free grammars to define syntax rules. Parsers build parse trees or abstract syntax trees (ASTs). Common parsing techniques include LL and LR parsing.","claim":"Parsing uses regular expressions to define syntax","gold_label":"CONTRADICT","evidence_span":{"start":77,"end":131}}
{"doc_id":"compilers_006","domain_topic":"Compilers","source_text":"Syntax analysis (parsing) checks if tokens form valid grammatical structures. It uses context-free grammars to define syntax rules. Parsers build parse trees or abstract syntax trees (ASTs). Common parsing techniques include LL and LR parsing.","claim":"Parsers build abstract syntax trees","gold_label":"ENTAIL","evidence_span":{"start":132,"end":193}}
{"doc_id":"compilers_007","domain_topic":"Compilers","source_text":"Semantic analysis checks for semantic errors and enforces language rules. It includes type checking, variable declaration checking, and scope resolution. A symbol table stores information about identifiers.","claim":"Semantic analysis includes type checking","gold_label":"ENTAIL","evidence_span":{"start":73,"end":147}}
{"doc_id":"compilers_008","domain_topic":"Compilers","source_text":"Semantic analysis checks for semantic errors and enforces language rules. It includes type checking, variable declaration checking, and scope resolution. A symbol table stores information about identifiers.","claim":"Semantic analysis produces machine code","gold_label":"NEUTRAL"}
{"doc_id":"compilers_009","domain_topic":"Compilers","source_text":"Intermediate code generation produces platform-independent representations. Three-address code is a common intermediate representation. It simplifies optimization and retargeting to different architectures.","claim":"Three-address code is an intermediate representation","gold_label":"ENTAIL","evidence_span":{"start":74,"end":132}}
{"doc_id":"compilers_010","domain_topic":"Compilers","source_text":"Code optimization improves program efficiency without changing semantics. Common optimizations include constant folding, dead code elimination, and loop optimization. Peephole optimization improves small code sequences.","claim":"Code optimization preserves program semantics","gold_label":"ENTAIL","evidence_span":{"start":0,"end":72}}
{"doc_id":"compilers_011","domain_topic":"Compilers","source_text":"Code optimization improves program efficiency without changing semantics. Common optimizations include constant folding, dead code elimination, and loop optimization. Peephole optimization improves small code sequences.","claim":"Code optimization changes program behavior","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":72}}
{"doc_id":"compilers_012","domain_topic":"Compilers","source_text":"Register allocation assigns variables to CPU registers. It uses graph coloring to minimize register spills. Registers are much faster than memory access.","claim":"Register allocation uses graph coloring","gold_label":"ENTAIL","evidence_span":{"start":54,"end":107}}
{"doc_id":"compilers_013","domain_topic":"Compilers","source_text":"LL parsers parse from left to right using leftmost derivations. They use a stack and predictive parsing table. LL(1) parsers look ahead one token.","claim":"LL parsers use leftmost derivations","gold_label":"ENTAIL","evidence_span":{"start":0,"end":63}}
{"doc_id":"compilers_014","domain_topic":"Compilers","source_text":"LL parsers parse from left to right using leftmost derivations. They use a stack and predictive parsing table. LL(1) parsers look ahead one token.","claim":"LL parsers use rightmost derivations","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":63}}
{"doc_id":"compilers_015","domain_topic":"Compilers","source_text":"LR parsers are more powerful than LL parsers and can handle a larger class of grammars. They use shift-reduce parsing and a parsing table. LR(1) parsers have one token lookahead.","claim":"LR parsers are more powerful than LL parsers","gold_label":"ENTAIL","evidence_span":{"start":0,"end":87}}
{"doc_id":"compilers_016","domain_topic":"Compilers","source_text":"LR parsers are more powerful than LL parsers and can handle a larger class of grammars. They use shift-reduce parsing and a parsing table. LR(1) parsers have one token lookahead.","claim":"LR parsers use recursive descent","gold_label":"CONTRADICT","evidence_span":{"start":89,"end":138}}
{"doc_id":"compilers_017","domain_topic":"Compilers","source_text":"Static typing checks types at compile time before execution. It catches type errors early and can improve performance. Dynamic typing checks types at runtime.","claim":"Static typing checks types at compile time","gold_label":"ENTAIL","evidence_span":{"start":0,"end":60}}
{"doc_id":"compilers_018","domain_topic":"Compilers","source_text":"Static typing checks types at compile time before execution. It catches type errors early and can improve performance. Dynamic typing checks types at runtime.","claim":"Static typing checks types at runtime","gold_label":"CONTRADICT","evidence_span":{"start":0,"end":60}}
{"doc_id":"compilers_019","domain_topic":"Compilers","source_text":"Just-In-Time (JIT) compilation compiles code during execution rather than beforehand. It combines benefits of interpretation and ahead-of-time compilation. JIT can perform runtime optimizations based on execution patterns.","claim":"JIT compiles code during execution","gold_label":"ENTAIL","evidence_span":{"start":0,"end":85}}
{"doc_id":"compilers_020","domain_topic":"Compilers","source_text":"Garbage collection automatically manages memory by reclaiming unused objects. Mark-and-sweep identifies reachable objects and frees others. Reference counting tracks object references but can\u0027t handle cycles.","claim":"Mark-and-sweep is a garbage collection algorithm","gold_label":"ENTAIL","evidence_span":{"start":78,"end":145}}
{"doc_id":"compilers_021","domain_topic":"Compilers","source_text":"Garbage collection automatically manages memory by reclaiming unused objects. Mark-and-sweep identifies reachable objects and frees others. Reference counting tracks object references but can\u0027t handle cycles.","claim":"Reference counting handles circular references","gold_label":"CONTRADICT","evidence_span":{"start":146,"end":208}}
{"doc_id":"compilers_022","domain_topic":"Compilers","source_text":"Abstract syntax trees (AST) represent the hierarchical structure of source code. They omit syntactic details like parentheses and semicolons. ASTs are used for semantic analysis and code generation.","claim":"ASTs represent hierarchical code structure","gold_label":"ENTAIL","evidence_span":{"start":0,"end":80}}
{"doc_id":"compilers_023","domain_topic":"Compilers","source_text":"Control flow graphs (CFG) represent program flow with nodes for basic blocks and edges for control flow. They\u0027re used for optimization and data flow analysis.","claim":"CFGs represent program control flow","gold_label":"ENTAIL","evidence_span":{"start":0,"end":106}}
{"doc_id":"compilers_024","domain_topic":"Compilers","source_text":"Constant folding evaluates constant expressions at compile time. For example, 2 * 3 becomes 6. It reduces runtime computation.","claim":"Constant folding evaluates expressions at compile time","gold_label":"ENTAIL","evidence_span":{"start":0,"end":65}}
{"doc_id":"compilers_025","domain_topic":"Compilers","source_text":"Dead code elimination removes unreachable or unused code. It reduces program size and improves performance. Compilers identify code that never executes.","claim":"Dead code elimination removes unused code","gold_label":"ENTAIL","evidence_span":{"start":0,"end":58}}
