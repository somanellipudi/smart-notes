{"doc_id": "algo_001", "domain_topic": "algorithms.sorting", "source_text": "Bubble sort repeatedly steps through the list, compares adjacent elements and swaps them if they are in wrong order. The pass through the list is repeated until the list is sorted. The worst-case time complexity is O(n²) where n is the number of elements.", "generated_claim": "Bubble sort has a worst-case time complexity of O(n²)", "gold_label": "VERIFIED", "evidence_span": "The worst-case time complexity is O(n²) where n is the number of elements"}
{"doc_id": "algo_002", "domain_topic": "algorithms.sorting", "source_text": "Quicksort is an efficient, divide-and-conquer sorting algorithm. It works by selecting a pivot element and partitioning the array around it. The average-case time complexity is O(n log n), and it has good cache locality making it practical for real-world use.", "generated_claim": "Quicksort always runs in O(n log n) time", "gold_label": "LOW_CONFIDENCE", "evidence_span": "The average-case time complexity is O(n log n)"}
{"doc_id": "algo_003", "domain_topic": "algorithms.sorting", "source_text": "Merge sort is a stable sorting algorithm that divides the array in half recursively and then merges the sorted halves back together. It has guaranteed O(n log n) time complexity in all cases: best, average, and worst-case.", "generated_claim": "Merge sort guarantees O(n log n) time complexity in all cases", "gold_label": "VERIFIED", "evidence_span": "It has guaranteed O(n log n) time complexity in all cases"}
{"doc_id": "ds_001", "domain_topic": "datastructures.hashtable", "source_text": "Hash tables provide constant-time O(1) average-case lookup, insertion, and deletion. However, in the worst case when many collisions occur, these operations can degrade to O(n). Good hash functions and appropriate load factors minimize collision probability.", "generated_claim": "Hash tables provide O(1) lookup in all cases", "gold_label": "REJECTED", "evidence_span": "in the worst case when many collisions occur, these operations can degrade to O(n)"}
{"doc_id": "ds_002", "domain_topic": "datastructures.binarytree", "source_text": "A balanced binary search tree maintains an invariant that the height difference between left and right subtrees is at most 1. This ensures that search, insertion, and deletion operations complete in O(log n) time. Examples include AVL trees and Red-Black trees.", "generated_claim": "Balanced binary search trees maintain height invariant for O(log n) operations", "gold_label": "VERIFIED", "evidence_span": "This ensures that search, insertion, and deletion operations complete in O(log n) time"}
{"doc_id": "ds_003", "domain_topic": "datastructures.graph", "source_text": "Breadth-first search (BFS) explores a graph level by level, visiting all nodes at distance k before visiting nodes at distance k+1. It uses a queue data structure and has time complexity O(V + E) where V is vertices and E is edges.", "generated_claim": "BFS has time complexity O(V + E)", "gold_label": "VERIFIED", "evidence_span": "time complexity O(V + E) where V is vertices and E is edges"}
{"doc_id": "ds_004", "domain_topic": "datastructures.graph", "source_text": "Depth-first search (DFS) explores a graph by going as deep as possible along each branch before backtracking. It can be implemented recursively using the call stack or iteratively using an explicit stack. Time complexity is O(V + E).", "generated_claim": "DFS explores each vertex and edge exactly once with O(V + E) complexity", "gold_label": "VERIFIED", "evidence_span": "Time complexity is O(V + E)"}
{"doc_id": "comp_001", "domain_topic": "complexity.nphard", "source_text": "The Traveling Salesman Problem (TSP) is NP-hard, meaning there is no known polynomial-time algorithm to solve it optimally. TSP requires finding the shortest route visiting all cities exactly once and returning to the starting city.", "generated_claim": "TSP is NP-hard", "gold_label": "VERIFIED", "evidence_span": "The Traveling Salesman Problem (TSP) is NP-hard"}
{"doc_id": "comp_002", "domain_topic": "complexity.correlation", "source_text": "Many NP-complete problems can be solved using dynamic programming or backtracking algorithms when the input size is small. However, these solutions typically still require exponential time in the worst case.", "generated_claim": "All NP-complete problems can be solved in polynomial time using dynamic programming", "gold_label": "REJECTED", "evidence_span": ""}
{"doc_id": "dp_001", "domain_topic": "algorithms.dynamicprogramming", "source_text": "The longest common subsequence (LCS) problem can be solved using dynamic programming. It builds a table where entry (i,j) represents the length of LCS of the first i characters of string 1 and first j characters of string 2. This gives O(m*n) time complexity.", "generated_claim": "LCS using dynamic programming has O(m*n) time complexity where m and n are string lengths", "gold_label": "VERIFIED", "evidence_span": "This gives O(m*n) time complexity"}
{"doc_id": "dp_002", "domain_topic": "algorithms.dynamicprogramming", "source_text": "The edit distance (Levenshtein distance) measures the minimum number of single-character edits needed to transform one string into another. Using dynamic programming, it can be computed in O(m*n) time and O(m*n) space.", "generated_claim": "Edit distance can be computed in O(m*n) time and space using dynamic programming", "gold_label": "VERIFIED", "evidence_span": "it can be computed in O(m*n) time and O(m*n) space"}
{"doc_id": "net_001", "domain_topic": "networking.basics", "source_text": "TCP (Transmission Control Protocol) is a connection-oriented protocol that provides reliable, ordered delivery of data. It uses a three-way handshake to establish connections: SYN, SYN-ACK, ACK. TCP ensures no data loss through sequence numbers and acknowledgments.", "generated_claim": "TCP provides reliable, ordered data delivery using connection establishment and acknowledgments", "gold_label": "VERIFIED", "evidence_span": "TCP (Transmission Control Protocol) is a connection-oriented protocol that provides reliable, ordered delivery of data"}
{"doc_id": "net_002", "domain_topic": "networking.protocol", "source_text": "UDP (User Datagram Protocol) is a connectionless protocol that provides fast transmission without guaranteeing delivery order or reliability. It has lower overhead than TCP and is suitable for applications like streaming where occasional packet loss is acceptable.", "generated_claim": "UDP guarantees packet delivery and maintains order", "gold_label": "REJECTED", "evidence_span": "provides fast transmission without guaranteeing delivery order or reliability"}
{"doc_id": "sec_001", "domain_topic": "security.encryption", "source_text": "RSA is an asymmetric encryption algorithm based on the difficulty of factoring large numbers. It uses a public key for encryption and a private key for decryption. The security of RSA depends on key length; 2048-bit keys are currently considered secure.", "generated_claim": "RSA uses public-key cryptography with security based on large number factorization", "gold_label": "VERIFIED", "evidence_span": "RSA is an asymmetric encryption algorithm based on the difficulty of factoring large numbers"}
{"doc_id": "sec_002", "domain_topic": "security.hashing", "source_text": "Cryptographic hash functions like SHA-256 take an input and produce a fixed-size hash output. They have properties: deterministic, quick computation, avalanche effect (small input change produces very different hash), and computationally infeasible to find collisions.", "generated_claim": "SHA-256 makes it practically impossible to find two inputs with the same hash", "gold_label": "VERIFIED", "evidence_span": "computationally infeasible to find collisions"}
{"doc_id": "db_001", "domain_topic": "databases.indexing", "source_text": "Database indexes, such as B-trees, enable fast lookup of records. A B-tree maintains sorted order and has logarithmic search time O(log n). Indexes speed up queries but require additional storage and maintenance overhead during insertions and deletions.", "generated_claim": "B-tree indexes provide O(log n) search time in databases", "gold_label": "VERIFIED", "evidence_span": "has logarithmic search time O(log n)"}
{"doc_id": "db_002", "domain_topic": "databases.sqlquery", "source_text": "In SQL, a JOIN operation combines rows from two tables based on a join condition. INNER JOIN returns matching rows from both tables, LEFT JOIN returns all rows from left table with matches from right, and CROSS JOIN returns the Cartesian product.", "generated_claim": "INNER JOIN returns all combinations of rows from both tables", "gold_label": "REJECTED", "evidence_span": "INNER JOIN returns matching rows from both tables"}
{"doc_id": "ml_001", "domain_topic": "machinelearning.optimization", "source_text": "Gradient descent is an optimization algorithm that iteratively moves toward the minimum of a cost function by taking steps proportional to the negative of the gradient. Learning rate controls step size; too high causes divergence, too low causes slow convergence.", "generated_claim": "Gradient descent converges with any learning rate value", "gold_label": "REJECTED", "evidence_span": "too high causes divergence"}
{"doc_id": "ml_002", "domain_topic": "machinelearning.regression", "source_text": "Linear regression finds the best-fit line through data points by minimizing the sum of squared errors. For multiple variables, it generalizes to linear least squares. The solution is unique if the feature matrix has full column rank.", "generated_claim": "Linear regression minimizes sum of squared errors to find the best fit", "gold_label": "VERIFIED", "evidence_span": "minimizing the sum of squared errors"}
{"doc_id": "cc_001", "domain_topic": "compilerscomputing.parsing", "source_text": "A compiler lexer converts source code into tokens. A parser then builds an abstract syntax tree (AST) from tokens following grammar rules. Semantic analysis checks type correctness and variable declarations. Code generation converts AST to machine code or intermediate representation.", "generated_claim": "A parser converts source code directly to machine code", "gold_label": "REJECTED", "evidence_span": ""}
{"doc_id": "cc_002", "domain_topic": "compilerscomputing.optimization", "source_text": "Compiler optimizations like constant folding replace expressions with constant values at compile time. Dead code elimination removes unreachable code. Loop unrolling replicates loop bodies to reduce loop overhead. These optimizations improve runtime performance without changing program semantics.", "generated_claim": "Compiler constant folding evaluates expressions at compile time for optimization", "gold_label": "VERIFIED", "evidence_span": "constant folding replace expressions with constant values at compile time"}
