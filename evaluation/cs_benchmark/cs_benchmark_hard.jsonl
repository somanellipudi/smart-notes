{"doc_id": "hard_001", "domain_topic": "algorithms.correctness", "source_text": "Quicksort is an efficient sorting algorithm with O(n log n) average-case complexity. However, in the worst case when the pivot selection consistently divides the array into unbalanced partitions, the complexity degrades to O(n^2). This happens particularly with already sorted or reverse-sorted arrays unless randomization is used.", "generated_claim": "Quicksort always has O(n log n) time complexity", "gold_label": "REJECTED", "evidence_span": "degrade to O(n^2)", "difficulty": "medium", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Subtle: average case vs worst case distinction"}}
{"doc_id": "hard_002", "domain_topic": "datastructures.btree", "source_text": "B-trees maintain balance after insertions through splitting nodes when they exceed maximum capacity. A B-tree of order m can contain at most m-1 keys per node. When a node reaches m keys, it's split into two nodes, with the median key promoted to the parent. This splitting operation ensures the tree remains balanced and all leaves stay at the same depth. Because B-trees are balanced, search takes O(log n) time.", "generated_claim": "B-trees guarantee O(log n) search time by maintaining balance", "gold_label": "VERIFIED", "evidence_span": "Because B-trees are balanced, search takes O(log n) time.", "difficulty": "hard", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Explicit evidence for logarithmic search"}}
{"doc_id": "hard_003", "domain_topic": "algorithms.searching", "source_text": "Binary search requires the input array to be sorted beforehand. The algorithm compares the target with the middle element and recursively searches the appropriate half. Each comparison eliminates half of the remaining elements, resulting in O(log n) time complexity. If the array is unsorted, binary search may return incorrect results or miss valid elements entirely.", "generated_claim": "Binary search works correctly on unsorted arrays with O(log n) time", "gold_label": "REJECTED", "evidence_span": "If the array is unsorted, binary search may return incorrect results", "difficulty": "hard", "source_type": "paper", "claim_type": "performance", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Common misconception"}}
{"doc_id": "hard_004", "domain_topic": "datastructures.heap", "source_text": "A min-heap is a complete binary tree where each parent node contains a value less than or equal to its children. This property is called the heap property. While a min-heap efficiently supports extract-min in O(1) time for the root, the heap structure only guarantees O(1) access to the minimum through the root node, not sorting of all elements.", "generated_claim": "Min-heaps store all elements in sorted order", "gold_label": "REJECTED", "evidence_span": "the heap structure only guarantees O(1) access to the minimum through the root node, not sorting of all elements", "difficulty": "medium", "source_type": "textbook", "claim_type": "definition", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Partial truth: only guarantees partial order"}}
{"doc_id": "hard_005", "domain_topic": "complexity.reductions", "source_text": "NP-complete problems are known to be in both NP and NP-hard classes. This means they are at least as hard as any problem in NP. Whether NP-complete problems have polynomial-time solutions remains unknown; this is the P vs NP problem. If any single NP-complete problem were solved in polynomial time, all NP problems could be solved in polynomial time.", "generated_claim": "NP-complete problems have polynomial time solutions", "gold_label": "REJECTED", "evidence_span": "Whether NP-complete problems have polynomial-time solutions remains unknown", "difficulty": "hard", "source_type": "paper", "claim_type": "performance", "reasoning_type": "multi_hop", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Open problem, not proven"}}
{"doc_id": "hard_006", "domain_topic": "networking.protocols", "source_text": "TCP uses acknowledgments to ensure reliable delivery of data. When a sender transmits a packet, it waits for an acknowledgment from the receiver. If no acknowledgment arrives within a timeout period, the sender retransmits the packet. However, duplicate packets can be safely discarded by the receiver using sequence numbers.", "generated_claim": "TCP can deliver duplicate packets to the application layer", "gold_label": "REJECTED", "evidence_span": "duplicate packets can be safely discarded by the receiver using sequence numbers", "difficulty": "medium", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Subtle: handled at transport layer"}}
{"doc_id": "hard_007", "domain_topic": "databases.normalization", "source_text": "Database normalization is the process of organizing data to minimize redundancy. First Normal Form (1NF) requires that all attributes contain atomic values only. Second Normal Form (2NF) requires that the table be in 1NF and all non-key attributes must be fully functionally dependent on the primary key. Partial dependencies indicate lack of 2NF compliance.", "generated_claim": "A table in 1NF is automatically in 2NF if it has a single-column primary key", "gold_label": "REJECTED", "evidence_span": "all non-key attributes must be fully functionally dependent on the primary key", "difficulty": "hard", "source_type": "textbook", "claim_type": "definition", "reasoning_type": "multi_hop", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Common misunderstanding"}}
{"doc_id": "hard_008", "domain_topic": "security.cryptography", "source_text": "RSA encryption security relies on the computational difficulty of factoring large semiprime numbers. If a polynomial-time factorization algorithm were discovered, RSA would become insecure. However, quantum computers using Shor's algorithm could efficiently factor large numbers, making RSA vulnerable to quantum attacks. RSA is currently secure against known classical computational attacks based on factoring.", "generated_claim": "RSA is currently secure against all known computational attacks", "gold_label": "VERIFIED", "evidence_span": "RSA is currently secure against known classical computational attacks based on factoring.", "difficulty": "hard", "source_type": "paper", "claim_type": "definition", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Explicit statement of classical security"}}
{"doc_id": "hard_009", "domain_topic": "algorithms.dynamic_programming", "source_text": "Dynamic programming solves problems by breaking them into overlapping subproblems and storing results to avoid recomputation. The optimal substructure property means the optimal solution contains optimal solutions to subproblems. Without both properties, dynamic programming may not be applicable or may produce suboptimal results.", "generated_claim": "Any problem with overlapping subproblems can be solved optimally with dynamic programming", "gold_label": "REJECTED", "evidence_span": "Without both properties, dynamic programming may not be applicable or may produce suboptimal results", "difficulty": "hard", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Requires both properties"}}
{"doc_id": "hard_010", "domain_topic": "machinelearning.generalization", "source_text": "Overfitting occurs when a model learns training data patterns including noise rather than generalizable features. A model with high training accuracy but low test accuracy is likely overfitting. Regularization techniques like L1/L2 penalize model complexity to reduce overfitting risk. However, increasing model capacity can sometimes improve generalization if properly regularized.", "generated_claim": "Larger models always overfit more than smaller models on the same dataset", "gold_label": "REJECTED", "evidence_span": "increasing model capacity can sometimes improve generalization if properly regularized", "difficulty": "hard", "source_type": "paper", "claim_type": "performance", "reasoning_type": "multi_hop", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Double descent phenomenon"}}
{"doc_id": "hard_011", "domain_topic": "compilers.parsing", "source_text": "Context-free grammars (CFG) can describe most programming language syntax. LL(1) parsers cannot handle left-recursive grammars directly, requiring left recursion elimination during grammar transformation. LR parsers can handle left-recursive grammars naturally, making them more powerful for grammar specification while requiring more complex parsing table generation. LR parsers can parse any context-free language, while LL parsers cannot.", "generated_claim": "LR parsers can parse any context-free language while LL parsers cannot", "gold_label": "VERIFIED", "evidence_span": "LR parsers can parse any context-free language, while LL parsers cannot.", "difficulty": "hard", "source_type": "textbook", "claim_type": "definition", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Explicit comparison statement"}}
{"doc_id": "hard_012", "domain_topic": "algorithms.graph_theory", "source_text": "In an undirected graph, a Hamiltonian path visits every vertex exactly once. Finding a Hamiltonian path is NP-complete, meaning no known polynomial-time algorithm exists for general graphs. However, for specific graph structures like complete graphs or graphs with special properties, polynomial-time algorithms may exist.", "generated_claim": "Finding a Hamiltonian path is NP-complete even for complete graphs", "gold_label": "REJECTED", "evidence_span": "for specific graph structures like complete graphs or graphs with special properties, polynomial-time algorithms may exist", "difficulty": "hard", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "NP-completeness for general case"}}
{"doc_id": "hard_013", "domain_topic": "networking.congestion_control", "source_text": "TCP congestion control adjusts transmission rates based on network conditions. Slow Start increases the congestion window exponentially until a threshold is reached. Congestion Avoidance increases the window linearly to probe for additional capacity. When packet loss is detected, the congestion window is reduced dramatically. Slow Start begins with a small congestion window, which is why it is called slow start.", "generated_claim": "TCP Slow Start is called slow because it begins with a small congestion window", "gold_label": "VERIFIED", "evidence_span": "Slow Start begins with a small congestion window, which is why it is called slow start.", "difficulty": "medium", "source_type": "paper", "claim_type": "definition", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Explicit naming rationale"}}
{"doc_id": "hard_014", "domain_topic": "security.authentication", "source_text": "Salting a password hash involves adding random data before hashing to ensure identical passwords produce different hashes. This defeats precomputed hash tables (rainbow tables). Each password should use a unique salt stored alongside its hash. However, salting does not increase the computational work required per hash attempt during brute force attack.", "generated_claim": "Salting passwords makes brute force attacks computationally harder", "gold_label": "REJECTED", "evidence_span": "salting does not increase the computational work required per hash attempt during brute force attack", "difficulty": "hard", "source_type": "textbook", "claim_type": "security", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Salt prevents precomputed tables but not brute force"}}
{"doc_id": "hard_015", "domain_topic": "datastructures.trie", "source_text": "A trie (prefix tree) stores strings using a tree where each node represents a character. Common prefixes share nodes, reducing space for similar strings. Search, insertion, and deletion take O(m) time where m is the key length, independent of the number of keys. This makes tries efficient for prefix matching and autocomplete applications.", "generated_claim": "Tries use more space than hash tables for storing small numbers of strings", "gold_label": "REJECTED", "evidence_span": "Common prefixes share nodes, reducing space for similar strings", "difficulty": "medium", "source_type": "textbook", "claim_type": "performance", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Space efficiency of tries with prefixes"}}
{"doc_id": "hard_016", "domain_topic": "algorithms.approximation", "source_text": "Approximation algorithms provide solutions with guaranteed bounds relative to optimal. A factor-k approximation guarantees the solution is at most k times worse than optimal. Some NP-hard problems like Vertex Cover have constant-factor approximations, while others like TSP have worse guarantees unless P=NP.", "generated_claim": "All NP-hard problems have polynomial-time constant-factor approximation algorithms", "gold_label": "REJECTED", "evidence_span": "while others like TSP have worse guarantees unless P=NP", "difficulty": "hard", "source_type": "paper", "claim_type": "performance", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Approximability varies by problem"}}
{"doc_id": "hard_017", "domain_topic": "databases.transactions", "source_text": "ACID guarantees ensure data integrity in transactions. Atomicity ensures all-or-nothing execution. Consistency maintains valid database state. Isolation prevents interference between concurrent transactions. Durability persists committed data. However, some distributed databases relax these guarantees for performance and availability.", "generated_claim": "All databases guarantee strict ACID properties at all times", "gold_label": "REJECTED", "evidence_span": "some distributed databases relax these guarantees for performance and availability", "difficulty": "hard", "source_type": "textbook", "claim_type": "definition", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "CAP theorem trade-offs"}}
{"doc_id": "hard_018", "domain_topic": "machinelearning.backpropagation", "source_text": "Backpropagation computes gradients of loss with respect to weights using the chain rule. The algorithm efficiently calculates all gradients in one backward pass. However, backpropagation requires computing intermediate activations, which consumes memory proportional to network depth.", "generated_claim": "Backpropagation is both time and space efficient with no trade-offs", "gold_label": "REJECTED", "evidence_span": "consumes memory proportional to network depth", "difficulty": "medium", "source_type": "paper", "claim_type": "performance", "reasoning_type": "direct", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Memory vs time trade-off"}}
{"doc_id": "hard_019", "domain_topic": "algorithms.hashing", "source_text": "Hash functions distribute keys uniformly to minimize collisions. A good hash function appears random while being deterministic. Load factor (items/slots) affects collision probability: higher load factors increase collisions. Collision resolution through chaining or open addressing maintains functionality despite collisions.", "generated_claim": "A perfect hash function eliminates all collisions regardless of load factor", "gold_label": "REJECTED", "evidence_span": "Load factor affects collision probability: higher load factors increase collisions", "difficulty": "medium", "source_type": "textbook", "claim_type": "definition", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Pigeonhole principle applies"}}
{"doc_id": "hard_020", "domain_topic": "compilers.optimization", "source_text": "Dead code elimination removes unreachable or unused code instructions. This optimization reduces binary size and improves cache efficiency. Constant folding pre-computes expressions with constant operands. Loop unrolling replicates loop bodies to reduce branching overhead. Each optimization trades off compile time against runtime improvement.", "generated_claim": "Compiler optimizations always reduce both binary size and runtime", "gold_label": "REJECTED", "evidence_span": "Each optimization trades off compile time against runtime improvement", "difficulty": "medium", "source_type": "paper", "claim_type": "performance", "reasoning_type": "implicit", "metadata": {"creation_date": "2026-02-17", "verifier": "research", "notes": "Size vs speed trade-offs exist"}}
